{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **목차**\n",
        "\n",
        "- 참고 할만한 사이트\n",
        "- 참고 할만한 논문\n",
        "- 자료\n",
        "- 전체적인 코드\n",
        "- 하찮은 변명...;;;"
      ],
      "metadata": {
        "id": "4SNt-A_J8ozE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **참고할만한 사이트**\n",
        "**1. Hugging Face**\n",
        " - Transformer 모델 라이브러리와 튜토리얼\n",
        " - URL: https://huggingface.co/transformers/\n",
        "\n",
        "**2. TensorFlow Hub**\n",
        "\n",
        " - 사전 훈련된 모델을 검색하고 사용하는 방법에 대한 코드 예제와 튜토리얼을 제공\n",
        " - URL: https://www.tensorflow.org/hub\n",
        "\n",
        "**3. PyTorch Hub**\n",
        "\n",
        " - PyTorch 기반의 사전 훈련된 모델을 쉽게 가져오고 사용할 수 있도록 지원\n",
        " - URL: https://pytorch.org/hub/\n",
        "\n",
        "**4. Towards Data Science**\n",
        "\n",
        " - 다양한 머신러닝 및 딥러닝 튜토리얼, 구현 방법, 코드 예제를 블로그 형식으로 제공\n",
        " - URL: https://towardsdatascience.com/\n",
        "\n",
        "**5. Machine Learning Mastery**\n",
        "\n",
        " - 머신러닝 및 딥러닝 모델의 구현과 관련된 다양한 튜토리얼과 코드 예제를 제공\n",
        " - URL: https://machinelearningmastery.com/\n",
        "\n",
        "**6. Analytics Vidhya**\n",
        "\n",
        " - 데이터 과학, 머신러닝, 딥러닝 관련 다양한 튜토리얼과 코드 예제를 제공\n",
        " - URL: https://www.analyticsvidhya.com/\n",
        "\n",
        "**7. Stack Overflow**\n",
        "\n",
        " - 다양한 프로그래밍 문제와 그 해결책이 공유되는 커뮤니티. 특정 문제나 구현에 대한 코드 예제를 찾기 좋음\n",
        " - URL: https://stackoverflow.com/\n",
        "\n",
        "**8. V Memoir**\n",
        " - 간단한 코드로 chatbot을 간단하게 만드는 사이트\n",
        " - URL: http://velog.io/@kimhwangdae/간단한-챗봇을-만들어보자\n",
        "\n",
        "**9. Medium**\n",
        " - Simple Chatbot using BERT and Pytorch: Part 1\n",
        "\n",
        " - https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa\n",
        "\n",
        "\n",
        "# **참고할만한 논문**\n",
        "**1. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**\n",
        "\n",
        " - URL: https://arxiv.org/abs/1810.04805\n",
        " - 논문 요약\n",
        "  - 이 논문에서는 BERT(Bidirectional Encoder Representations from Transformers) 모델을 소개\n",
        "  - BERT는 양방향으로 트랜스포머를 학습시켜 문맥의 양방향성을 최대한 활용하는 모델\n",
        "  - 이를 통해 다양한 자연어 처리 작업에서 뛰어난 성능을 보여주며, 사전 학습된 BERT 모델을 특정 작업에 맞게 미세 조정하여 높은 성능을 발휘\n",
        "\n",
        "**2. Attention Is All You Need**\n",
        "\n",
        " - URL: https://arxiv.org/abs/1706.03762\n",
        " - 논문 요약\n",
        "  - 이 논문은 트랜스포머(Transformer) 모델을 소개하며, 기존의 순환 신경망(RNN)이나 합성곱 신경망(CNN) 없이도 주목(attention) 메커니즘만으로 뛰어난 성능을 발휘할 수 있음을 보여줌.\n",
        "  - 트랜스포머는 병렬 처리가 가능하며, 번역, 문장 요약 등의 다양한 자연어 처리 작업에서 뛰어난 성능을 발휘\n",
        "\n",
        "**3. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**\n",
        "\n",
        " - URL: https://arxiv.org/abs/1910.01108\n",
        " - 논문 요약:\n",
        "  - DistilBERT는 BERT의 경량화 버전으로, 모델 크기와 학습 시간을 줄이면서도 성능을 유지하는 데 중점을 둠\n",
        "  - 논문은 DistilBERT의 구조와 학습 방법을 설명하며, 원본 BERT와 비교하여 효율성을 강조\n",
        "\n",
        "**4. RoBERTa: A Robustly Optimized BERT Pretraining Approach**\n",
        "\n",
        " - URL: https://arxiv.org/abs/1907.11692\n",
        " - 논문 요약\n",
        "  - RoBERTa는 BERT의 사전 학습 과정을 개선한 모델로, 대규모 데이터셋에서 더 오래 학습하여 성능을 향상\n",
        "   - 논문은 RoBERTa의 학습 과정과 성능을 BERT와 비교하여 설명\n",
        "\n",
        "#**자료**\n",
        "**1. Deep Learning Specialization by Andrew Ng (Coursera)**\n",
        "\n",
        " - URL: https://www.coursera.org/specializations/deep-learning\n",
        " - 설명\n",
        "   - 유명한 딥러닝 전문가 Andrew Ng가 제공하는 딥러닝 과정으로, 신경망의 기본 개념부터 심화 학습까지 다룸\n",
        "\n",
        "**2. fast.ai**\n",
        " - URL: https://www.fast.ai/\n",
        " - 설명\n",
        "  - 딥러닝 및 머신러닝을 쉽게 배울 수 있도록 다양한 튜토리얼과 강의를 제공하는 사이트"
      ],
      "metadata": {
        "id": "TGTeBxG_aROU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 데이터 로드 및 전처리**"
      ],
      "metadata": {
        "id": "F8Z3h8ueplfP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm1hm_B6aDqW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 텍스트 데이터 로드 및 전처리 함수 정의\n",
        "def load_and_preprocess_data(file_path):\n",
        "    # CSV 파일에서 데이터 로드(file_path는 예시)\n",
        "    #df = pd.read_csv(file_path)\n",
        "\n",
        "    # 필요한 열 선택(x,y는 예시)\n",
        "    #df = df[['x', 'y']]\n",
        "\n",
        "    # 데이터 분할\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "    return train_texts, val_texts, train_labels, val_labels\n",
        "\n",
        "# 데이터 로드 및 전처리 함수 호출\n",
        "# train_texts, val_texts, train_labels, val_labels = load_and_preprocess_data('your_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. BERT 모델 파인튜닝**"
      ],
      "metadata": {
        "id": "HlQh14kHyOPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# 토크나이저 및 모델 로드\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Dataset 클래스를 정의\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 주어진 인덱스의 데이터 항목을 반환\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])  # 레이블을 텐서로 변환하여 추가\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        # 데이터셋의 크기를 반환\n",
        "        return len(self.labels)\n",
        "\n",
        "# 학습 데이터셋과 검증 데이터셋 생성\n",
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "val_dataset = TextDataset(val_encodings, val_labels)\n",
        "\n",
        "# 트레이닝 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',              # 모델 예측과 체크포인트가 저장될 디렉토리\n",
        "    num_train_epochs=3,                  # 학습 에폭 수\n",
        "    per_device_train_batch_size=8,       # 학습 배치 크기\n",
        "    per_device_eval_batch_size=8,        # 평가 배치 크기\n",
        "    warmup_steps=500,                    # 학습 초기에 학습률이 점진적으로 증가하는 단계 수\n",
        "    weight_decay=0.01,                   # 가중치 감쇠\n",
        "    logging_dir='./logs',                # 로그가 저장될 디렉토리\n",
        ")\n",
        "\n",
        "# Trainer 설정\n",
        "trainer = Trainer(\n",
        "    model=model,                         # 학습할 모델\n",
        "    args=training_args,                  # 트레이닝 설정\n",
        "    train_dataset=train_dataset,         # 학습 데이터셋\n",
        "    eval_dataset=val_dataset             # 평가 데이터셋\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "zbmWrIlH9XOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 모델 저장 및 로드**"
      ],
      "metadata": {
        "id": "JxnR6e0DyX5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model.save_pretrained('path_to_save_model')\n",
        "tokenizer.save_pretrained('path_to_save_tokenizer')\n",
        "\n",
        "# 모델 로드\n",
        "model = BertForSequenceClassification.from_pretrained('path_to_save_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('path_to_save_tokenizer')"
      ],
      "metadata": {
        "id": "_9aR2AhEydlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **완성도 및 테스트**\n",
        " - 수업 이후 완전 처음으로 짜는 거라 미흡합니다.\n",
        " - 여러 사이트 참고하여 큰 틀은 잡고 미흡한 부분은 GPT 사용했습니다.\n",
        " - 완성도는 아직입니다.\n",
        " - 데이터 로드 어떤걸로 해야 될지 몰라서 테스트는 아직 못 했습니다.\n",
        " - 보시고 부족한 부분 및 수정 부분이 있으면 말씀해 주세요."
      ],
      "metadata": {
        "id": "DlkmytLE2evR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xB8wI-Xw3AaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}