2024/7/2
1. 영화 관련 데이터 파일 업로드
2. chatgpt의 도움으로 시험 삼아 영화 대화 파일 1개로 모델을 만들어봄
3. 처음 CPU로 돌린 결과 10-15분 걸림
4. gpu로 돌린 결과 5초

2024/7/5
1. 87000의 대화로 모델을 만들어 봄
2. 5분정도 걸림
3. 300 epoch , batch=15,  Loss=0.005700
4. 299 epoch, batch=5,   Loss=0.001500

tokenizer = 'gpt2'
model = GPT2LMHeadModel.from_pretrained('gpt2')

결과
1)                                                 2)
Response: 추천 해 줄 영화 있어?
2집 근처에는 병원 �
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

답변은 여전히 안됨
gpt 3.5 모델을 사용하거나, 데이터를 어떻게 더 전처리 하는지 알아야 할 거 같음

Response: 아니 어제 머했냐고 20분 거리에 있어.
2집 

2024/7/8
KoGPT를 이용해서 만들어좀
걸린 시간 4시간

과정은 따로 설명하겠음..

결과
user > 범죄도시 재밌냐?
Chatbot > 나 그런 거 안 봤어
user > 범죄 도시 같은 영화 추천 해줘
Chatbot > 나 범죄 도시  제일 최근에 본 영화야
user > 다른 영화 없어?
Chatbot > 나 외국 영화 좋아하잖아~
user > 한국영화는 안 좋아하냐?
Chatbot > 응응 영화가 제일 재밌어
user > 어제는 모 했어?
Chatbot > 오늘부터인가
user > 아침 밥은 먹었어?
Chatbot > 응 근데 나 오메가쓰리 많이 먹어서 요즘 오메가쓰리 많이 먹는 중
user > 개킹받네
Chatbot > 그럼 그 강아지 쌉가능
user > 내놔 임마
Chatbot > 그럼 너가 대신 들어가 줄게
user > ㅋㅋㅋ
Chatbot > 아유 키키 그래도 이번 추석은 잘 버틸 수 있겠어


2024/7/12
파이토치 라이트닝 형식으로, 7/8일 코드를 바꿔보고 learningrate schduler를 이용해서 learningrate를 조정해봤으나 
유요한 효과는 보지 못함

2024/7/15
데이터 증식을 NLPAUG의 삭제 옵션으로 해봤으나 유의미한 결과가 안 나옴

2024/7/16
한국어 데이터를, 영어로 번역, 번역한 결과를 NLPAUG의 동의어 기능으로 증식 했으나 유의미한 결과가 안 나옴

2024/7/17
이 블로그의 코드를 이용해서 존댓 말로 바꿈

결과
user > 밥은 먹었니?
Chatbot > 오늘 점심 어떤 거 드셨어요.
user > 영화 추천 해줘
Chatbot > 나는 모가디슈가 젤 재밌어요.
user > 얌마 말좀 제대로 해봐
Chatbot > 네 저 진짜 편해서 너가 저희 가자고 하면 바로 달려갈 듯해요.

생각보다 잘 나옴


출처:
https://github.com/jongmin-oh/korean-formal-classifier
https://velog.io/@acdongpgm/NLP.-%ED%95%9C%EA%B5%AD%EC%96%B4Korean%EC%97%90%EC%84%9C-%EB%B0%98%EB%A7%90informal%EC%9D%84-%EC%A1%B4%EB%8C%93%EB%A7%90formal%EB%A1%9C-%EB%B0%94%EA%BF%94%EC%A3%BC%EB%8A%94-%EB%B3%80%ED%99%98%EA%B8%B0convertor-korean-Formal-Convertor-Using-Deep-Learning
https://velog.io/@acdongpgm/NLP.-%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%A1%B4%EB%8C%93%EB%A7%90%EB%B0%98%EB%A7%90-%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8-formal-classifier