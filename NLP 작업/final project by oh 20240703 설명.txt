# 토크나이저 설정
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  #는 Hugging Face의 Transformers 라이브러리에서 제공하는 코드로, 
GPT-2 모델에 맞는 토크나이저를 로드,from_pretrained 메서드 사용: 이 메서드는 지정된 이름('gpt2')에 해당하는 사전 훈련된 토크나이저를 다운로드하고 로드합니다.


tokenizer.pad_token = tokenizer.eos_token
라는 코드는 GPT-2 모델의 토크나이저에서 패딩 토큰과 종료 토큰을 동일하게 설정

패딩 토큰 (pad_token): 일반적으로 시퀀스 데이터를 같은 길이로 맞추기 위해 사용되는 토큰입니다. 이는 배치 처리를 할 때 유용합니다.
종료 토큰 (eos_token): 문장의 끝을 나타내는 토큰으로, 모델이 텍스트 생성을 멈추는 기준으로 사용됩니다.
[나는, 사과를, 좋아합니다, pad_token, pad_token] 마지막 pad_token 무시하여 떠 빠른 연산.


# 토큰화된 데이터 생성
inputs = tokenizer(dataset_text, return_tensors='pt', max_length=512, truncation=True, padding='max_length') 코드는 주어진 텍스트 데이터를 
PyTorch 텐서 형식으로 변환하고, 길이를 최대 512 토큰으로 제한하며, 패딩을 적용하여 고정된 길이로 만듭니다.

코드 설명
dataset_text: 입력 텍스트 데이터입니다. 일반적으로 문자열이나 문자열의 리스트입니다.
tokenizer: Hugging Face의 토크나이저 객체입니다.
return_tensors='pt': 토크나이저가 반환하는 값을 PyTorch 텐서로 변환합니다. TensorFlow 텐서를 원하면 'tf'를 사용하면 됩니다.
max_length=512: 최대 시퀀스 길이를 512 토큰으로 제한합니다.
truncation=True: 텍스트가 최대 길이를 초과할 경우 잘라냅니다.
padding='max_length': 모든 시퀀스를 최대 길이로 패딩합니다. 즉, 길이가 부족한 시퀀스는 max_length에 맞춰 패딩됩니다.


위 코드는 inputs 변수에 토크나이저가 변환한 텍스트 데이터가 PyTorch 텐서 형식으로 저장됩니다. inputs는 일반적으로 다음과 같은 키를 포함하는 딕셔너리입니다:
input_ids: 텍스트를 토큰 ID로 변환한 결과.
attention_mask: 실제 텍스트와 패딩을 구분하는 마스크 (실제 텍스트는 1, 패딩은 0).

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, inputs):
        self.input_ids = inputs['input_ids']

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'labels': self.input_ids[idx]
        }

dataset = CustomDataset(inputs)

CustomDataset 클래스는 입력 데이터에서 input_ids를 추출하여 모델 학습에 필요한 형식으로 준비하는 사용자 정의 데이터셋
데이터를 pytorch의 DataLoader와 함꼐 사용되어 배치 단위로 데이터를 모델에 공급할 수 있게 함


data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
배치 데이터를 준비하는 데 사용되는 도구로, 모델에 입력될 데이터를 일정한 형식으로 변환합니다.
일반적으로 패딩(padding), 정렬, 마스킹 등의 작업을 수행합니다.
언어 모델 학습을 위한 데이터 준비 과정에서, 주어진 텍스트 데이터에서 입력과 레이블을 생성하는 작업을 포함합니다.
DataCollatorForLanguageModeling은 입력 데이터를 배치로 정리하고, 필요한 패딩을 추가하여 일관된 길이로 만들어줍니다.



 


# 훈련 설정
training_args = TrainingArguments(
    output_dir='./results',              훈련 결과(모델 가중치, 설정, 로그 등)가 저장될 디렉토리 경로를 지정합니다.
    num_train_epochs=3,			
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    logging_dir='./logs', 로그 파일이 저장될 디렉토리 경로를 지정합니다.
    logging_steps=10,
    save_steps=500,몇 스텝마다 모델을 저장할지 지정합니다.
    warmup_steps=100, 학습 초기에 학습률을 천천히 증가시키는 워밍업 스텝 수를 지정합니다. 이는 학습률 스케줄링 전략의 일부입니다.
    weight_decay=0.01,  가중치 감쇠율을 지정합니다. 이는 과적합을 방지하기 위한 정규화 기법으로, 모델의 가중치가 너무 커지지 않도록 제어합니다.
    prediction_loss_only=True 평가 시 예측 손실만 계산할지 여부를 지정합니다. 이 설정이 True이면, 평가 동안 손실만 계산하고 다른 메트릭은 계산하지 않습니다.
)


함수 정의
python
코드 복사
def generate_response(model, tokenizer, input_text, max_length=50):
model: 사전 훈련된 GPT-2 모델 객체.
tokenizer: 사전 훈련된 GPT-2 토크나이저 객체.
input_text: 응답을 생성할 입력 텍스트 문자열.
max_length: 생성할 응답의 최대 길이. 기본값은 50.
모델 평가 모드 설정
python
코드 복사
model.eval()
모델을 평가 모드로 전환합니다. 이는 드롭아웃(dropout)과 같은 훈련 시에만 사용되는 레이어를 비활성화합니다.
입력 텍스트 토큰화 및 장치로 이동
python
코드 복사
inputs = tokenizer.encode(input_text, return_tensors='pt').to(device)
tokenizer.encode: 입력 텍스트를 토큰화하여 토큰 ID로 변환합니다.
input_text: "어떤 영화를 추천해줄래?"와 같은 문자열 입력.
return_tensors='pt': 결과를 PyTorch 텐서로 반환합니다.
to(device): 생성된 텐서를 지정된 장치(GPU 또는 CPU)로 이동합니다.
응답 생성
python
코드 복사
with torch.no_grad():
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)
torch.no_grad(): 이 블록 내에서 그래디언트 계산을 비활성화하여 메모리 사용량을 줄이고 계산 속도를 높입니다.
model.generate: 모델이 입력 시퀀스(inputs)에 대한 응답을 생성합니다.
inputs: 토큰화된 입력 텍스트 텐서.
max_length: 생성할 응답의 최대 길이.
num_return_sequences: 생성할 응답의 개수. 여기서는 1개만 생성합니다.
pad_token_id: 패딩 토큰 ID. GPT-2에서는 종료 토큰 ID(tokenizer.eos_token_id)를 사용하여 패딩합니다.




두번쨰 시도
# 모델을 GPU로 이동
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

장치 설정: torch.device를 사용하여 GPU가 사용 가능한지 확인하고, GPU가 가능하면 "cuda"를 사용하고, 그렇지 않으면 "cpu"를 사용합니다.


data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
tokenizer: 사용할 토크나이저를 지정합니다. 이 경우에는 tokenizer 변수를 사용하여 이전에 정의된 토크나이저를 사용합니다.
mlm=False: 마스킹 언어 모델링(Masked Language Modeling)을 사용할지 여부를 설정합니다. False로 설정하면 GPT와 같은 일반적인 언어 모델을 학습하는 데 사용됩니다. True로 설정하면 BERT와 같은 마스킹 언어 모델을 학습하는 데 사용됩니다.


inputs = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length', return_tensors='pt')
targets = tokenizer(target_texts, max_length=512, truncation=True, padding='max_length', return_tensors='pt')
파라미터 설명
dataset_text:

설명: 토큰화할 입력 텍스트입니다. 보통 문자열이나 문자열의 리스트 형식입니다.
예시: ["Hello, how are you?", "I am fine, thank you!"]
return_tensors='pt':

설명: 토큰화된 출력을 PyTorch 텐서 형식으로 반환합니다. 'pt'는 PyTorch를 의미합니다.
대안: 텐서플로우 형식으로 반환하려면 'tf'를 사용합니다.
예시: {'input_ids': tensor([[15496, 11, 703, 389, 345, 30, 50256], [314, 716, 2477, 286, 262, 481, 30, 50256]])}
max_length=512:

설명: 토큰 시퀀스의 최대 길이를 512로 설정합니다. 시퀀스가 이 길이를 초과하면 잘리게 됩니다.
예시: 텍스트가 매우 길 경우, 최대 512 토큰까지만 사용합니다.
truncation=True:

설명: 텍스트가 최대 길이(max_length)를 초과하면 잘라냅니다.
예시: "This is a very long sentence that will be truncated if it exceeds the maximum length set for the tokenizer."
padding='max_length':

설명: 시퀀스의 길이가 max_length보다 짧으면, 최대 길이에 맞추기 위해 패딩을 추가합니다.
대안: 'longest'를 사용하면 배치 내 가장 긴 시퀀스에 맞춰 패딩합니다.
예시: 패딩 토큰이 추가되어 시퀀스 길이가 512가 됩니다.


#코드 설명
generate_response 함수: 챗봇 응답을 생성하는 함수입니다.
model: 사전 훈련된 언어 모델 (예: GPT-2).
tokenizer: 모델에 맞는 토크나이저.
input_text: 사용자가 입력한 텍스트.
max_length: 생성할 응답의 최대 길이 (기본값: 50).
주요 단계
모델 평가 모드 설정: model.eval()을 통해 모델을 평가 모드로 전환합니다. 이는 드롭아웃과 같은 레이어를 비활성화합니다.
입력 텍스트 토큰화: tokenizer.encode를 사용하여 입력 텍스트를 토큰화하고, PyTorch 텐서로 변환합니다. 이를 모델이 실행되는 장치 (device)로 전송합니다.
응답 생성: model.generate를 사용하여 모델이 응답을 생성합니다. 여기서 max_length는 응답의 최대 길이를 설정하고, num_return_sequences는 생성할 응답의 수를 지정하며, pad_token_id는 패딩 토큰의 ID를 설정합니다.
응답 디코딩: tokenizer.decode를 사용하여 생성된 토큰을 사람이 읽을 수 있는 텍스트로 변환합니다.


=True

gpt통해서 대화 만들기             