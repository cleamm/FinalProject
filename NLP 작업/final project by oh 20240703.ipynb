{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNcsBrovWPJRqkyz+JnCa23"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"257173c7b61e4e53993fe970e7b6543a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88119055987147208ffbd17bfc626d4d","IPY_MODEL_344b9fbda07d4884a7f2a8f861e8f6c1","IPY_MODEL_04655295115946459d5c9ca1697c8ff3"],"layout":"IPY_MODEL_5d0114e663984b58b815f68b8c2afa56"}},"88119055987147208ffbd17bfc626d4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_797d3076f53c40e7ab1c05b27f9087df","placeholder":"​","style":"IPY_MODEL_10bd733669a848488266260920825083","value":"model.safetensors: 100%"}},"344b9fbda07d4884a7f2a8f861e8f6c1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_83b600e018334e2493e033a2e8d5995f","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66851462ffec48f4bc7e282ed8ff389e","value":440449768}},"04655295115946459d5c9ca1697c8ff3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bfcd0b3bb3f47198a103b55ecd6fb24","placeholder":"​","style":"IPY_MODEL_9b9603cf870944ea80509cecf0f4eb8a","value":" 440M/440M [00:06&lt;00:00, 105MB/s]"}},"5d0114e663984b58b815f68b8c2afa56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"797d3076f53c40e7ab1c05b27f9087df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10bd733669a848488266260920825083":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83b600e018334e2493e033a2e8d5995f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66851462ffec48f4bc7e282ed8ff389e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bfcd0b3bb3f47198a103b55ecd6fb24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b9603cf870944ea80509cecf0f4eb8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install transformers[torch]\n","!pip install transformers[torch] accelerate -U datasets torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CqGdicvGAm9","executionInfo":{"status":"ok","timestamp":1719982516435,"user_tz":-540,"elapsed":185689,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"3c4d2d5e-a5ec-4a64-e1ec-14829cabccc2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Collecting transformers[torch]\n","  Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n","Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Collecting torch\n","  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests (from transformers[torch])\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Collecting triton==2.3.1 (from torch)\n","  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, triton, requests, pyarrow, dill, multiprocess, torch, transformers, datasets\n","  Attempting uninstall: triton\n","    Found existing installation: triton 2.3.0\n","    Uninstalling triton-2.3.0:\n","      Successfully uninstalled triton-2.3.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.0+cu121\n","    Uninstalling torch-2.3.0+cu121:\n","      Successfully uninstalled torch-2.3.0+cu121\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.2\n","    Uninstalling transformers-4.41.2:\n","      Successfully uninstalled transformers-4.41.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n","torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\n","torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 torch-2.3.1 transformers-4.42.3 triton-2.3.1 xxhash-3.4.1\n"]}]},{"cell_type":"code","source":["!pip install torch transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HAHM4NDMb1k","executionInfo":{"status":"ok","timestamp":1719986680557,"user_tz":-540,"elapsed":10762,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"74aad7fd-7847-4d73-f4d5-876cbaf80872"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from google.colab import drive"],"metadata":{"id":"UmRV6ajLMr3h","executionInfo":{"status":"ok","timestamp":1719986749941,"user_tz":-540,"elapsed":14530,"user":{"displayName":"크복짜","userId":"12410456823605642737"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","data_path = r'C:\\Users\\Oh\\final project\\movie dialogue'\n","\n","if not os.path.exists(data_path):\n","    print(f\"The directory {data_path} does not exist.\")\n","else:\n","    print(f\"The directory {data_path} exists.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IAc-mAGv_-oz","executionInfo":{"status":"ok","timestamp":1719983404721,"user_tz":-540,"elapsed":520,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"d06756b7-23de-4b76-b8ce-8516e081b179"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["The directory C:\\Users\\Oh\\final project\\movie dialogue does not exist.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVacbsztAfi7","executionInfo":{"status":"ok","timestamp":1719983606652,"user_tz":-540,"elapsed":59547,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"f2c142aa-de09-4c51-ffd2-9e8f624dd30f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","from transformers import AutoTokenizer\n","\n","# 구글 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 데이터 경로 설정 (구글 드라이브 내의 경로)\n","data_path = '/content/drive/MyDrive/movie dialogue'\n","\n","# 모든 JSON 파일의 경로를 리스트로 저장\n","json_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.json')]\n","\n","# 데이터를 저장할 리스트\n","all_texts = []\n","\n","# 각 JSON 파일을 불러와서 데이터 추가\n","for file_path in json_files:\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","        for item in data['info']:\n","            text = item['annotations']['text']\n","            all_texts.append(text)\n","\n","print(f'Loaded {len(all_texts)} dialogues.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wP0J1HQsD-jP","executionInfo":{"status":"ok","timestamp":1719986575864,"user_tz":-540,"elapsed":62208,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"f2360ee8-0e00-49b0-e1b5-c8570b576dab"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loaded 4276 dialogues.\n"]}]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","# 텍스트 데이터를 추출하여 토큰화\n","tokenized_texts = [tokenizer(text, truncation=True, padding='max_length', max_length=512) for text in all_texts]\n","\n","# 토큰화된 데이터를 저장할 리스트\n","input_ids = [tokens['input_ids'] for tokens in tokenized_texts]\n","attention_masks = [tokens['attention_mask'] for tokens in tokenized_texts]\n","\n","# 데이터셋 생성\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, input_ids, attention_masks):\n","        self.input_ids = input_ids\n","        self.attention_masks = attention_masks\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': torch.tensor(self.input_ids[idx]),\n","            'attention_mask': torch.tensor(self.attention_masks[idx])\n","        }\n","\n","dataset = CustomDataset(input_ids, attention_masks)"],"metadata":{"id":"Ru2YhUO9KfVz","executionInfo":{"status":"ok","timestamp":1719986759871,"user_tz":-540,"elapsed":5366,"user":{"displayName":"크복짜","userId":"12410456823605642737"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107,"referenced_widgets":["257173c7b61e4e53993fe970e7b6543a","88119055987147208ffbd17bfc626d4d","344b9fbda07d4884a7f2a8f861e8f6c1","04655295115946459d5c9ca1697c8ff3","5d0114e663984b58b815f68b8c2afa56","797d3076f53c40e7ab1c05b27f9087df","10bd733669a848488266260920825083","83b600e018334e2493e033a2e8d5995f","66851462ffec48f4bc7e282ed8ff389e","7bfcd0b3bb3f47198a103b55ecd6fb24","9b9603cf870944ea80509cecf0f4eb8a"]},"id":"tXvMeKf9LAPq","executionInfo":{"status":"ok","timestamp":1719986805300,"user_tz":-540,"elapsed":7916,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"76d7da5b-d3da-49f0-a3e6-c26719c85ebd"},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257173c7b61e4e53993fe970e7b6543a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# 훈련 설정\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n",")"],"metadata":{"id":"LlCdf3PcMHWh","executionInfo":{"status":"ok","timestamp":1719986838961,"user_tz":-540,"elapsed":796,"user":{"displayName":"크복짜","userId":"12410456823605642737"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["!pip install torch transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hx69dEjiNNoq","executionInfo":{"status":"ok","timestamp":1719987126980,"user_tz":-540,"elapsed":12306,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"f7ed382a-4a43-4cb3-a76c-7630965389c8"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","from google.colab import drive\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')\n","\n","# 데이터 경로 설정 (구글 드라이브 내의 경로)\n","data_path = '/content/drive/MyDrive/movie dialogue'\n","\n","# 모든 JSON 파일의 경로를 리스트로 저장\n","json_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.json')]\n","\n","# 데이터를 저장할 리스트\n","all_texts = []\n","\n","# 각 JSON 파일을 불러와서 데이터 추가\n","for file_path in json_files:\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","        for item in data['info']:\n","            text = item['annotations']['text']\n","            all_texts.append(text)\n","\n","print(f'Loaded {len(all_texts)} dialogues.')\n","\n","# 텍스트 데이터를 결합하여 GPT 모델 입력용 텍스트 생성\n","dataset_text = \"\\n\".join(all_texts)\n","\n","# 토크나이저 설정\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# 토큰화된 데이터 생성\n","inputs = tokenizer(dataset_text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs):\n","        self.input_ids = inputs['input_ids']\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.input_ids[idx],\n","            'labels': self.input_ids[idx]\n","        }\n","\n","dataset = CustomDataset(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4yzB31BOI78","executionInfo":{"status":"ok","timestamp":1719990504941,"user_tz":-540,"elapsed":68959,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"75c12ef9-d248-4073-909c-23ae3cacc355"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loaded 4276 dialogues.\n"]}]},{"cell_type":"code","source":["# 모델 설정\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# 데이터 콜레이터 설정\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","# 훈련 설정\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    save_steps=500,\n","    warmup_steps=100,\n","    weight_decay=0.01,\n","    prediction_loss_only=True\n",")\n","\n","# Trainer 설정\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n","\n","# 모델 학습\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":115},"id":"0fSDXKD8ONpD","executionInfo":{"status":"ok","timestamp":1719990523278,"user_tz":-540,"elapsed":14070,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"f5163a20-95c7-4e08-fe7c-8b509aa27b94"},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:11, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=3, training_loss=2.6267995834350586, metrics={'train_runtime': 11.6385, 'train_samples_per_second': 0.258, 'train_steps_per_second': 0.258, 'total_flos': 783876096000.0, 'train_loss': 2.6267995834350586, 'epoch': 3.0})"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# 챗봇 응답 생성 함수\n","def generate_response(model, tokenizer, input_text, max_length=50):\n","    model.eval()\n","    inputs = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    with torch.no_grad():\n","        outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n"],"metadata":{"id":"Oj6BmGqAOqMv","executionInfo":{"status":"ok","timestamp":1719990545519,"user_tz":-540,"elapsed":559,"user":{"displayName":"크복짜","userId":"12410456823605642737"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# 예시 입력 텍스트\n","input_text = \"어떤 영화를 추천해줄래?\"\n","\n","# 모델을 사용하여 응답 생성\n","response = generate_response(model, tokenizer, input_text)\n","print(f\"Response: {response}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTDDVTWNO3rq","executionInfo":{"status":"ok","timestamp":1719990566843,"user_tz":-540,"elapsed":1231,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"38ec185d-7c20-4913-ce90-c12cccfbafa8"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Response: 어떤 영화를 추천해줄래?\n","\n","어떤 영화를 추\n"]}]},{"cell_type":"code","source":["# 모델을 GPU로 이동\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeO5BwsrPvCB","executionInfo":{"status":"ok","timestamp":1719987535822,"user_tz":-540,"elapsed":1334,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"cbbc78c7-7a60-45d1-ae71-1d8821b04f89"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# 모델 설정\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# 모델을 GPU로 이동\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 데이터 콜레이터 설정\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","# 훈련 설정\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=100,\n","    per_device_train_batch_size=30,\n","    per_device_eval_batch_size=30,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    save_steps=500,\n","    warmup_steps=100,\n","    weight_decay=0.01,\n","    prediction_loss_only=True,\n",")\n","\n","# Trainer 설정\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")\n","\n","# 모델 학습\n","trainer.train()\n","\n","# 챗봇 응답 생성 함수\n","def generate_response(model, tokenizer, input_text, max_length=50):\n","    model.eval()\n","    inputs = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    with torch.no_grad():\n","        outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","# 예시 입력 텍스트\n","input_text = \"어떤 영화를 추천해줄래?\"\n","\n","# 모델을 사용하여 응답 생성\n","response = generate_response(model, tokenizer, input_text)\n","print(f\"Response: {response}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"id":"BDzmjBFoSHCg","executionInfo":{"status":"ok","timestamp":1719988219908,"user_tz":-540,"elapsed":31532,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"94ea1df8-b3b1-459d-cac1-9407c76f6f9f"},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 00:26, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.591900</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.364700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.080700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.754100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.406700</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.003400</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.642700</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.364100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.197300</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.122600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Response: 어떤 영화를 추천해줄래?\n","어떤 잭! 아니지�\n"]}]},{"cell_type":"code","source":["len(json_files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pb-nl5zSQA7B","executionInfo":{"status":"ok","timestamp":1719987881208,"user_tz":-540,"elapsed":500,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"7b8bc81a-471e-41de-a624-1c305d5bbeef"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4276"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["len(all_texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ct41BqkZQ59D","executionInfo":{"status":"ok","timestamp":1719987918498,"user_tz":-540,"elapsed":483,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"f904fcdd-2848-496f-ee5e-e2e6c7965071"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4276"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["!pip install torch transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kieJipqhTY4F","executionInfo":{"status":"ok","timestamp":1719990681793,"user_tz":-540,"elapsed":6064,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"adb7272e-97e1-47d5-e375-b0db3a191262"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from google.colab import drive\n","\n","# 구글 드라이브 마운트\n","drive.mount('/content/drive')\n","\n","# 데이터 경로 설정 (구글 드라이브 내의 경로)\n","data_path = '/content/drive/MyDrive/movie dialogue'\n","\n","# 모든 JSON 파일의 경로를 리스트로 저장\n","json_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.json')]\n","\n","# 데이터를 저장할 리스트\n","dialogue_pairs = []\n","\n","# 각 JSON 파일을 불러와서 데이터 추가\n","for file_path in json_files:\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","        for item in data['info']:\n","            text = item['annotations']['text']\n","            # 대화 쌍 추출 (여기서는 간단히 첫 번째 줄과 두 번째 줄을 대화 쌍으로 가정)\n","            lines = text.split('\\n')\n","            if len(lines) >= 2:\n","                dialogue_pairs.append((lines[0], lines[1]))\n","\n","print(f'Loaded {len(dialogue_pairs)} dialogue pairs.')\n","\n","# 토크나이저 설정\n","tokenizer = AutoTokenizer.from_pretrained('t5-small')\n","\n","# 대화 데이터를 추출하여 토큰화\n","input_texts = [pair[0] for pair in dialogue_pairs]\n","target_texts = [pair[1] for pair in dialogue_pairs]\n","\n","# 토큰화된 데이터를 저장할 리스트\n","inputs = tokenizer(input_texts, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n","targets = tokenizer(target_texts, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n","\n","# 데이터셋 생성\n","class DialogueDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, targets):\n","        self.inputs = inputs\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.inputs['input_ids'])\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'input_ids': self.inputs['input_ids'][idx],\n","            'attention_mask': self.inputs['attention_mask'][idx],\n","            'labels': self.targets['input_ids'][idx]\n","        }\n","\n","dataset = DialogueDataset(inputs, targets)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TtvmfkT0bua6","executionInfo":{"status":"ok","timestamp":1719990989538,"user_tz":-540,"elapsed":15611,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"435fc5ec-d742-4525-ffbf-a208f7bc73fc"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loaded 4276 dialogue pairs.\n"]}]},{"cell_type":"code","source":["# 모델 설정\n","model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n","\n","# 훈련 전 캐시 초기화\n","torch.cuda.empty_cache()\n","\n","# 훈련 설정\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,  # 배치 크기를 줄임\n","    per_device_eval_batch_size=8,   # 배치 크기를 줄임\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    gradient_accumulation_steps=4,  # Gradient Accumulation 사용\n","    fp16=True  # Mixed Precision Training 사용\n",")\n","\n","# Trainer 설정\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n",")\n","\n","# 모델 학습\n","trainer.train()\n","\n","# 훈련 후 캐시 초기화\n","torch.cuda.empty_cache()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"d0wLlBoDcg4Z","executionInfo":{"status":"ok","timestamp":1719991588081,"user_tz":-540,"elapsed":549619,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"6d322e7d-1977-419b-9a3a-0ba70f2057d5"},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='399' max='399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [399/399 09:05, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>15.419900</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>15.656500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>15.454400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>15.225600</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>13.983400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>13.101200</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>12.100000</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>10.330800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>8.197200</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>6.347500</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>4.780100</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>3.109600</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>2.144600</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>1.343500</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.934800</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.728200</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.514500</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.312100</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.210700</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.123300</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.108400</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.080300</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.073800</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.071400</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.063400</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.057000</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.052600</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.047100</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.044900</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.041300</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.037900</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.032500</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.029200</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.025800</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.023000</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.021600</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.020900</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.019700</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.019700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["def generate_response(model, tokenizer, input_text):\n","    # 모델이 사용 중인 장치 가져오기\n","    device = next(model.parameters()).device\n","\n","    # 입력 텍스트를 토큰화하고 텐서를 모델이 위치한 장치로 이동\n","    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(device)\n","\n","    # 응답 생성\n","    with torch.no_grad():\n","        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=50)\n","\n","    # 출력 텐서를 디코딩하여 응답 생성\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","# 예시 입력 텍스트\n","input_text = \"어떤 영화를 추천해줄래?\"\n","\n","# 모델을 사용하여 응답 생성\n","response = generate_response(model, tokenizer, input_text)\n","print(f\"Response: {response}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_02MrhobwjR","executionInfo":{"status":"ok","timestamp":1719991853086,"user_tz":-540,"elapsed":1306,"user":{"displayName":"크복짜","userId":"12410456823605642737"}},"outputId":"f6b93368-101d-421b-d2b1-a5f49b50d500"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Response:     \n"]}]},{"cell_type":"code","source":["#추천 하는건 머신러닝이고\n","# 대화를 하는 건 ..."],"metadata":{"id":"EFgMxHIRfOee"},"execution_count":null,"outputs":[]}]}