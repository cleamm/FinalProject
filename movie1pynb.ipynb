{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cleamm/FinalProject/blob/master/movie1pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37MIkkAYZPp5",
        "outputId": "1591036a-fe72-4d29-dad8-f38da22bb49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdyE9HVMaS2h",
        "outputId": "72566296-796d-42ba-c8b1-565a81fb6830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl xlrd konlpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqhZ6Jc1aS0W",
        "outputId": "0d8b0836-9bae-409f-9dd9-1d1fba1e5341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (8,575 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 121920 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Jb34DxnaSyN",
        "outputId": "033bbc88-c8bc-413f-f0cd-39915b7f092c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "83/83 [==============================] - 47s 316ms/step - loss: 8.1136 - accuracy: 0.0000e+00 - val_loss: 8.1360 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "83/83 [==============================] - 22s 268ms/step - loss: 8.0884 - accuracy: 3.7722e-04 - val_loss: 8.2032 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/30\n",
            "83/83 [==============================] - 27s 330ms/step - loss: 8.0719 - accuracy: 0.0000e+00 - val_loss: 8.3750 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/30\n",
            "83/83 [==============================] - 18s 216ms/step - loss: 8.0216 - accuracy: 7.5443e-04 - val_loss: 8.6365 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/30\n",
            "83/83 [==============================] - 18s 221ms/step - loss: 7.8778 - accuracy: 0.0000e+00 - val_loss: 9.1263 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/30\n",
            "83/83 [==============================] - 19s 230ms/step - loss: 7.7190 - accuracy: 0.0000e+00 - val_loss: 9.7148 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/30\n",
            "83/83 [==============================] - 18s 217ms/step - loss: 7.6126 - accuracy: 0.0011 - val_loss: 10.1512 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/30\n",
            "83/83 [==============================] - 20s 238ms/step - loss: 7.5191 - accuracy: 3.7722e-04 - val_loss: 10.5008 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/30\n",
            "83/83 [==============================] - 21s 255ms/step - loss: 7.4175 - accuracy: 7.5443e-04 - val_loss: 10.6904 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/30\n",
            "83/83 [==============================] - 20s 238ms/step - loss: 7.3681 - accuracy: 0.0011 - val_loss: 11.1436 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/30\n",
            "83/83 [==============================] - 18s 215ms/step - loss: 7.3098 - accuracy: 0.0011 - val_loss: 11.1845 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/30\n",
            "83/83 [==============================] - 18s 217ms/step - loss: 7.2530 - accuracy: 0.0011 - val_loss: 11.4847 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/30\n",
            "83/83 [==============================] - 20s 237ms/step - loss: 7.1891 - accuracy: 0.0015 - val_loss: 11.3716 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/30\n",
            "83/83 [==============================] - 18s 217ms/step - loss: 7.1584 - accuracy: 0.0023 - val_loss: 11.6202 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/30\n",
            "83/83 [==============================] - 25s 297ms/step - loss: 7.1147 - accuracy: 0.0019 - val_loss: 11.8846 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/30\n",
            "83/83 [==============================] - 20s 242ms/step - loss: 7.0782 - accuracy: 0.0030 - val_loss: 12.0460 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/30\n",
            "83/83 [==============================] - 20s 243ms/step - loss: 7.0613 - accuracy: 0.0026 - val_loss: 12.0128 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/30\n",
            "83/83 [==============================] - 18s 221ms/step - loss: 7.0072 - accuracy: 0.0011 - val_loss: 12.3064 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/30\n",
            "83/83 [==============================] - 19s 225ms/step - loss: 6.9750 - accuracy: 0.0041 - val_loss: 12.3686 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/30\n",
            "83/83 [==============================] - 20s 238ms/step - loss: 6.9417 - accuracy: 0.0034 - val_loss: 12.5075 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/30\n",
            "83/83 [==============================] - 18s 219ms/step - loss: 6.9179 - accuracy: 0.0023 - val_loss: 12.7879 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/30\n",
            "83/83 [==============================] - 20s 236ms/step - loss: 6.8794 - accuracy: 0.0026 - val_loss: 12.7024 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/30\n",
            "83/83 [==============================] - 20s 245ms/step - loss: 6.8647 - accuracy: 0.0034 - val_loss: 12.8246 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/30\n",
            "83/83 [==============================] - 18s 219ms/step - loss: 6.8384 - accuracy: 0.0030 - val_loss: 12.8980 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/30\n",
            "83/83 [==============================] - 20s 241ms/step - loss: 6.8225 - accuracy: 0.0034 - val_loss: 12.7455 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/30\n",
            "83/83 [==============================] - 18s 220ms/step - loss: 6.7845 - accuracy: 0.0049 - val_loss: 13.0830 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/30\n",
            "83/83 [==============================] - 20s 241ms/step - loss: 6.7604 - accuracy: 0.0034 - val_loss: 13.0666 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/30\n",
            "83/83 [==============================] - 19s 226ms/step - loss: 6.7437 - accuracy: 0.0049 - val_loss: 13.4230 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/30\n",
            "83/83 [==============================] - 18s 222ms/step - loss: 6.7244 - accuracy: 0.0072 - val_loss: 13.2908 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/30\n",
            "83/83 [==============================] - 21s 257ms/step - loss: 6.7025 - accuracy: 0.0064 - val_loss: 13.5163 - val_accuracy: 0.0000e+00\n",
            "영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): 아이언맨 2\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "추천된 영화들:\n",
            " [['둠스데이 지구 최후의 날', 'SF', '미국', 2011.0], ['틴에이지 크라켄 루비', '애니메이션', '미국', 2023.0], ['극장판 쥬얼펫: 스위트댄스 프린세스', '애니메이션', '일본', 2012.0], ['고 피쉬!', '애니메이션,어드벤처', '미국', 2019.0], ['23 아이덴티티', '스릴러', '미국', 2016.0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46176 (\\N{HANGUL SYLLABLE DUM}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49828 (\\N{HANGUL SYLLABLE SEU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45936 (\\N{HANGUL SYLLABLE DE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44396 (\\N{HANGUL SYLLABLE GU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52572 (\\N{HANGUL SYLLABLE COE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54980 (\\N{HANGUL SYLLABLE HU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51032 (\\N{HANGUL SYLLABLE YI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45216 (\\N{HANGUL SYLLABLE NAL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54004 (\\N{HANGUL SYLLABLE TIN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 53356 (\\N{HANGUL SYLLABLE KEU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46972 (\\N{HANGUL SYLLABLE RA}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52996 (\\N{HANGUL SYLLABLE KEN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47336 (\\N{HANGUL SYLLABLE RU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44537 (\\N{HANGUL SYLLABLE GEUG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51109 (\\N{HANGUL SYLLABLE JANG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54032 (\\N{HANGUL SYLLABLE PAN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51564 (\\N{HANGUL SYLLABLE JYU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50620 (\\N{HANGUL SYLLABLE EOL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54187 (\\N{HANGUL SYLLABLE PES}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50948 (\\N{HANGUL SYLLABLE WI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45828 (\\N{HANGUL SYLLABLE DAEN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54532 (\\N{HANGUL SYLLABLE PEU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47536 (\\N{HANGUL SYLLABLE RIN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44256 (\\N{HANGUL SYLLABLE GO}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54588 (\\N{HANGUL SYLLABLE PI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49772 (\\N{HANGUL SYLLABLE SWI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45940 (\\N{HANGUL SYLLABLE DEN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54000 (\\N{HANGUL SYLLABLE TI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50689 (\\N{HANGUL SYLLABLE YEONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54868 (\\N{HANGUL SYLLABLE HWA}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52628 (\\N{HANGUL SYLLABLE CU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52380 (\\N{HANGUL SYLLABLE CEON}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50976 (\\N{HANGUL SYLLABLE YU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51216 (\\N{HANGUL SYLLABLE JEOM}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 파일 불러오기\n",
        "file_path = 'movie12.xls'\n",
        "data = pd.read_excel(file_path, engine='xlrd', header=None)\n",
        "\n",
        "# column 이름 지정\n",
        "column_names = [\n",
        "    \"movie_name\", \"movie_name_English\", \"year_of_production\",\n",
        "    \"country_of_production\", \"type\", \"genre\", \"production_status\",\n",
        "    \"director\", \"production_company\"\n",
        "]\n",
        "\n",
        "data.columns = column_names\n",
        "\n",
        "# NLP를 위한 텍스트 특성 결합\n",
        "data['Text'] = data['movie_name'] + ' ' + data['production_company'] + ' ' + data['genre'] + ' ' + data['director']\n",
        "\n",
        "# 한국어 텍스트 처리를 위한 Okt 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 텍스트를 토큰화하고 원형으로 복원하는 함수 정의\n",
        "def tokenize_and_lemmatize(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text, stem=True)\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# 텍스트 데이터에 함수 적용\n",
        "data['Processed_Text'] = data['Text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# 텍스트 데이터 인코딩 및 패딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Processed_Text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Processed_Text'])\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['movie_name'])\n",
        "\n",
        "# 모델 구축\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "def get_recommendations_dl(movie_title=None, genre=None, country=None, year=None):\n",
        "    if not movie_title and not genre and not country and not year:\n",
        "        return \"영화 제목, 장르, 제작 국가 또는 제작 연도 중 하나를 입력하세요.\"\n",
        "\n",
        "    if movie_title:\n",
        "        if movie_title not in data['movie_name'].values:\n",
        "            return \"영화 제목이 데이터베이스에 없습니다.\"\n",
        "\n",
        "        # 영화 제목에 해당하는 인덱스 가져오기\n",
        "        idx = data[data['movie_name'] == movie_title].index[0]\n",
        "        movie_sequence = pad_sequences(tokenizer.texts_to_sequences([data.loc[idx, 'Processed_Text']]), maxlen=max_sequence_length)\n",
        "        predictions = model.predict(movie_sequence)[0]\n",
        "\n",
        "        sim_scores = []\n",
        "        for i in range(len(predictions)):\n",
        "            sim_scores.append((i, predictions[i]))\n",
        "\n",
        "        # 유사도 순으로 정렬\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 상위 5개 추천\n",
        "        sim_scores = sim_scores[:5]\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        recommendations = data.iloc[movie_indices]\n",
        "\n",
        "        # 시각화를 위한 유사도 점수 포함\n",
        "        similarity_scores = [i[1] for i in sim_scores]\n",
        "        return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "    else:\n",
        "        # 필터링된 영화에서 상위 5개 선택\n",
        "        recommendations = data.head(5)\n",
        "        similarity_scores = [1] * len(recommendations)  # 필터링된 결과에서는 유사도 점수를 1로 설정\n",
        "\n",
        "    recommendations = recommendations.copy()\n",
        "    recommendations.loc[:, 'year_of_production'] = recommendations['year_of_production'].astype(int)\n",
        "    return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "# 사용자 입력을 통한 영화 제목, 장르, 제작 국가, 제작 연도 입력\n",
        "user_input = input(\"영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): \").strip()\n",
        "\n",
        "# 입력된 값을 쉼표로 구분하여 리스트로 변환\n",
        "inputs = [i.strip() if i.strip() else None for i in user_input.split(',')]\n",
        "inputs = inputs + [None] * (4 - len(inputs))  # 입력된 값의 개수가 4개가 되도록 보충\n",
        "movie_title, genre, country, year = inputs\n",
        "\n",
        "recommended_movies, similarity_scores = get_recommendations_dl(movie_title, genre, country, year)\n",
        "print(\"추천된 영화들:\\n\", recommended_movies)\n",
        "\n",
        "# 추천된 영화 목록과 유사도 점수를 시각화\n",
        "def visualize_recommendations(recommended_movies, similarity_scores):\n",
        "    # 추천된 영화 목록을 데이터프레임으로 변환\n",
        "    df = pd.DataFrame(recommended_movies, columns=[\"영화 제목\", \"장르\", \"제작 국가\", \"제작 연도\"])\n",
        "    df[\"유사도 점수\"] = similarity_scores\n",
        "\n",
        "    # 바 차트 생성\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(df[\"영화 제목\"], df[\"유사도 점수\"], color='skyblue')\n",
        "    plt.xlabel('유사도 점수')\n",
        "    plt.title('영화 추천 유사도 점수')\n",
        "    plt.gca().invert_yaxis()  # 영화 제목이 위에서 아래로 표시되도록 순서 뒤집기\n",
        "    for i, (title, score) in enumerate(zip(df[\"영화 제목\"], df[\"유사도 점수\"])):\n",
        "        plt.text(score, i, f'{score:.2f}', va='center')\n",
        "    plt.show()\n",
        "\n",
        "visualize_recommendations(recommended_movies, similarity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrB88K8r6ClF"
      },
      "outputs": [],
      "source": [
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# accuracy는 영화추천에 좋은 건아닌거같다 loss랑 optimizer도 다른 것들 사용해보자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Iufz4kJaSgu"
      },
      "outputs": [],
      "source": [
        "# model.compile(loss=root_mean_squared_error, optimizer=SGD(), metrics=[root_mean_squared_error])\n",
        "# rmse로 하고 optimizer는 sgd를 써봄 일딴 실행 속도너무느리다 lstm을 줄여야할뜻"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PKqklQJOfF1d",
        "outputId": "1adaf83a-490c-4ec4-a07a-888cd08e0839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "83/83 [==============================] - 82s 805ms/step - loss: 1881.7218 - root_mean_squared_error: 1881.9712 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 2/30\n",
            "83/83 [==============================] - 62s 752ms/step - loss: 1883.4418 - root_mean_squared_error: 1883.9274 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 3/30\n",
            "83/83 [==============================] - 62s 746ms/step - loss: 1883.1387 - root_mean_squared_error: 1882.9833 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 4/30\n",
            "52/83 [=================>............] - ETA: 25s - loss: 1891.3718 - root_mean_squared_error: 1891.3718"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# RMSE 정의\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, y_pred.dtype)  # y_true를 y_pred와 같은 데이터 유형으로 변환\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "# 파일 불러오기\n",
        "file_path = '/content/drive/MyDrive/movie12.xls'\n",
        "data = pd.read_excel(file_path, engine='xlrd', header=None)\n",
        "\n",
        "# 컬럼 이름 지정\n",
        "column_names = [\n",
        "    \"movie_name\", \"movie_name_English\", \"year_of_production\",\n",
        "    \"country_of_production\", \"type\", \"genre\", \"production_status\",\n",
        "    \"director\", \"production_company\"\n",
        "]\n",
        "\n",
        "data.columns = column_names\n",
        "\n",
        "# NLP를 위한 텍스트 특성 결합\n",
        "data['Text'] = data['movie_name'] + ' ' + data['production_company'] + ' ' + data['genre'] + ' ' + data['director']\n",
        "\n",
        "# 한국어 텍스트 처리를 위한 Okt 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 텍스트를 토큰화하고 원형으로 복원하는 함수 정의\n",
        "def tokenize_and_lemmatize(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text, stem=True)\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# 텍스트 데이터에 함수 적용\n",
        "data['Processed_Text'] = data['Text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# 텍스트 데이터 인코딩 및 패딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Processed_Text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Processed_Text'])\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['movie_name'])\n",
        "\n",
        "# 모델 구축\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=512, input_length=max_sequence_length))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# loss와 metrics에 RMSE 정의\n",
        "model.compile(loss=root_mean_squared_error, optimizer=SGD(), metrics=[root_mean_squared_error])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "def get_recommendations_dl(movie_title=None, genre=None, country=None, year=None):\n",
        "    if not movie_title and not genre and not country and not year:\n",
        "        return \"영화 제목, 장르, 제작 국가 또는 제작 연도 중 하나를 입력하세요.\"\n",
        "\n",
        "    if movie_title:\n",
        "        if movie_title not in data['movie_name'].values:\n",
        "            return \"영화 제목이 데이터베이스에 없습니다.\"\n",
        "\n",
        "        # 영화 제목에 해당하는 인덱스 가져오기\n",
        "        idx = data[data['movie_name'] == movie_title].index[0]\n",
        "        movie_sequence = pad_sequences(tokenizer.texts_to_sequences([data.loc[idx, 'Processed_Text']]), maxlen=max_sequence_length)\n",
        "        predictions = model.predict(movie_sequence)[0]\n",
        "\n",
        "        sim_scores = []\n",
        "        for i in range(len(predictions)):\n",
        "            sim_scores.append((i, predictions[i]))\n",
        "\n",
        "        # 유사도 순으로 정렬\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 상위 5개 추천\n",
        "        sim_scores = sim_scores[:5]\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        recommendations = data.iloc[movie_indices]\n",
        "\n",
        "        # 시각화를 위한 유사도 점수 포함\n",
        "        similarity_scores = [i[1] for i in sim_scores]\n",
        "        return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "    else:\n",
        "        # 필터링된 영화에서 상위 5개 선택\n",
        "        recommendations = data.head(5)\n",
        "        similarity_scores = [1] * len(recommendations)  # 필터링된 결과에서는 유사도 점수를 1로 설정\n",
        "\n",
        "    recommendations = recommendations.copy()\n",
        "    recommendations.loc[:, 'year_of_production'] = recommendations['year_of_production'].astype(int)\n",
        "    return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "# 사용자 입력을 통한 영화 제목, 장르, 제작 국가, 제작 연도 입력\n",
        "user_input = input(\"영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): \").strip()\n",
        "\n",
        "# 입력된 값을 쉼표로 구분하여 리스트로 변환\n",
        "inputs = [i.strip() if i.strip() else None for i in user_input.split(',')]\n",
        "inputs = inputs + [None] * (4 - len(inputs))  # 입력된 값의 개수가 4개가 되도록 보충\n",
        "movie_title, genre, country, year = inputs\n",
        "\n",
        "recommended_movies, similarity_scores = get_recommendations_dl(movie_title, genre, country, year)\n",
        "print(\"추천된 영화들:\\n\", recommended_movies)\n",
        "\n",
        "# # 추천된 영화 목록과 유사도 점수를 시각화\n",
        "# def visualize_recommendations(recommended_movies, similarity_scores):\n",
        "#     # 추천된 영화 목록을 데이터프레임으로 변환\n",
        "#     df = pd.DataFrame(recommended_movies, columns=[\"영화 제목\", \"장르\", \"제작 국가\", \"제작 연도\"])\n",
        "#     df[\"유사도 점수\"] = similarity_scores\n",
        "\n",
        "#     # 바 차트 생성\n",
        "#     plt.figure(figsize=(12, 6))\n",
        "#     plt.barh(df[\"영화 제목\"], df[\"유사도 점수\"], color='skyblue')\n",
        "#     plt.xlabel('유사도 점수')\n",
        "#     plt.title('영화 추천 유사도 점수')\n",
        "#     plt.gca().invert_yaxis()  # 영화 제목이 위에서 아래로 표시되도록 순서 뒤집기\n",
        "#     for i, (title, score) in enumerate(zip(df[\"영화 제목\"], df[\"유사도 점수\"])):\n",
        "#         plt.text(score, i, f'{score:.2f}', va='center')\n",
        "#     plt.show()\n",
        "\n",
        "# visualize_recommendations(recommended_movies, similarity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibF8QuNq2Fti"
      },
      "outputs": [],
      "source": [
        "#일딴 시각화 잠시 삭제\n",
        "#영화 실험조건 잠시 변경(제목만)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6pyItAlbzBxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f698f0c-08af-4614-ac0d-6d754c104528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "83/83 [==============================] - 20s 111ms/step - loss: 1882.6100 - root_mean_squared_error: 1881.8296 - val_loss: 1948.2717 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 2/100\n",
            "83/83 [==============================] - 4s 50ms/step - loss: 1881.8342 - root_mean_squared_error: 1882.2976 - val_loss: 1948.1725 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 3/100\n",
            "83/83 [==============================] - 3s 36ms/step - loss: 1884.0232 - root_mean_squared_error: 1883.7572 - val_loss: 1948.1410 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 4/100\n",
            "83/83 [==============================] - 2s 25ms/step - loss: 1884.0889 - root_mean_squared_error: 1884.2937 - val_loss: 1948.1327 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 5/100\n",
            "83/83 [==============================] - 3s 32ms/step - loss: 1882.8727 - root_mean_squared_error: 1882.8118 - val_loss: 1948.1310 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 6/100\n",
            "83/83 [==============================] - 3s 35ms/step - loss: 1883.0121 - root_mean_squared_error: 1883.4854 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 7/100\n",
            "83/83 [==============================] - 2s 28ms/step - loss: 1884.0848 - root_mean_squared_error: 1883.4746 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 8/100\n",
            "83/83 [==============================] - 2s 19ms/step - loss: 1882.7875 - root_mean_squared_error: 1882.6591 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 9/100\n",
            "83/83 [==============================] - 1s 17ms/step - loss: 1883.5793 - root_mean_squared_error: 1883.5167 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 10/100\n",
            "83/83 [==============================] - 1s 17ms/step - loss: 1882.7821 - root_mean_squared_error: 1883.1288 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 11/100\n",
            "83/83 [==============================] - 1s 18ms/step - loss: 1884.4893 - root_mean_squared_error: 1884.7239 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 12/100\n",
            "83/83 [==============================] - 2s 19ms/step - loss: 1882.0066 - root_mean_squared_error: 1881.4590 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 13/100\n",
            "83/83 [==============================] - 2s 28ms/step - loss: 1884.2820 - root_mean_squared_error: 1883.7633 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 14/100\n",
            "83/83 [==============================] - 2s 23ms/step - loss: 1883.6769 - root_mean_squared_error: 1883.6039 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 15/100\n",
            "83/83 [==============================] - 1s 17ms/step - loss: 1883.2676 - root_mean_squared_error: 1883.3336 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "Epoch 16/100\n",
            "83/83 [==============================] - 2s 19ms/step - loss: 1885.3605 - root_mean_squared_error: 1885.1451 - val_loss: 1948.1306 - val_root_mean_squared_error: 1954.1888\n",
            "영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): 아이언맨 2\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "추천된 영화들:\n",
            " [['위 스틸 스틸 더 올드 웨이', '액션,코미디,범죄', '영국', 2017.0], ['극장판 5등분의 신부', '애니메이션,드라마', '일본', 2022.0], ['라스트 크리스마스', '코미디,멜로/로맨스', '미국', 2019.0], ['라이어×라이어', '멜로/로맨스', '일본', 2021.0], ['퀸카가 아니어도 좋아', '멜로/로맨스,코미디', '미국', 2015.0]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# RMSE 정의\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, y_pred.dtype)  # y_true를 y_pred와 같은 데이터 유형으로 변환\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "# 파일 불러오기\n",
        "file_path = '/content/drive/MyDrive/movie12.xls'\n",
        "data = pd.read_excel(file_path, engine='xlrd', header=None)\n",
        "\n",
        "# 컬럼 이름 지정\n",
        "column_names = [\n",
        "    \"movie_name\", \"movie_name_English\", \"year_of_production\",\n",
        "    \"country_of_production\", \"type\", \"genre\", \"production_status\",\n",
        "    \"director\", \"production_company\"\n",
        "]\n",
        "\n",
        "data.columns = column_names\n",
        "\n",
        "# NLP를 위한 텍스트 특성 결합\n",
        "data['Text'] = data['movie_name'] + ' ' + data['production_company'] + ' ' + data['genre'] + ' ' + data['director']\n",
        "\n",
        "# 한국어 텍스트 처리를 위한 Okt 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 텍스트를 토큰화하고 원형으로 복원하는 함수 정의\n",
        "def tokenize_and_lemmatize(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text, stem=True)\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# 텍스트 데이터에 함수 적용\n",
        "data['Processed_Text'] = data['Text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# 텍스트 데이터 인코딩 및 패딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Processed_Text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Processed_Text'])\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['movie_name'])\n",
        "\n",
        "# 모델 구축\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256, input_length=max_sequence_length))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# loss와 metrics에 RMSE 정의\n",
        "model.compile(loss=root_mean_squared_error, optimizer=Adam(learning_rate=0.001), metrics=[root_mean_squared_error])\n",
        "\n",
        "# Early Stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(X, y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "def get_recommendations_dl(movie_title=None, genre=None, country=None, year=None):\n",
        "    if not movie_title and not genre and not country and not year:\n",
        "        return \"영화 제목, 장르, 제작 국가 또는 제작 연도 중 하나를 입력하세요.\"\n",
        "\n",
        "    if movie_title:\n",
        "        if movie_title not in data['movie_name'].values:\n",
        "            return \"영화 제목이 데이터베이스에 없습니다.\"\n",
        "\n",
        "        # 영화 제목에 해당하는 인덱스 가져오기\n",
        "        idx = data[data['movie_name'] == movie_title].index[0]\n",
        "        movie_sequence = pad_sequences(tokenizer.texts_to_sequences([data.loc[idx, 'Processed_Text']]), maxlen=max_sequence_length)\n",
        "        predictions = model.predict(movie_sequence)[0]\n",
        "\n",
        "        sim_scores = []\n",
        "        for i in range(len(predictions)):\n",
        "            sim_scores.append((i, predictions[i]))\n",
        "\n",
        "        # 유사도 순으로 정렬\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 상위 5개 추천\n",
        "        sim_scores = sim_scores[:5]\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        recommendations = data.iloc[movie_indices]\n",
        "\n",
        "        # 시각화를 위한 유사도 점수 포함\n",
        "        similarity_scores = [i[1] for i in sim_scores]\n",
        "        return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "    else:\n",
        "        # 필터링된 영화에서 상위 5개 선택\n",
        "        recommendations = data.head(5)\n",
        "        similarity_scores = [1] * len(recommendations)  # 필터링된 결과에서는 유사도 점수를 1로 설정\n",
        "\n",
        "    recommendations = recommendations.copy()\n",
        "    recommendations.loc[:, 'year_of_production'] = recommendations['year_of_production'].astype(int)\n",
        "    return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "# 사용자 입력을 통한 영화 제목, 장르, 제작 국가, 제작 연도 입력\n",
        "user_input = input(\"영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): \").strip()\n",
        "\n",
        "# 입력된 값을 쉼표로 구분하여 리스트로 변환\n",
        "inputs = [i.strip() if i.strip() else None for i in user_input.split(',')]\n",
        "inputs = inputs + [None] * (4 - len(inputs))  # 입력된 값의 개수가 4개가 되도록 보충\n",
        "movie_title, genre, country, year = inputs\n",
        "\n",
        "recommended_movies, similarity_scores = get_recommendations_dl(movie_title, genre, country, year)\n",
        "print(\"추천된 영화들:\\n\", recommended_movies)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 파일 불러오기\n",
        "file_path = '/content/drive/MyDrive/movie12.xls'\n",
        "data = pd.read_excel(file_path, engine='xlrd', header=None)\n",
        "\n",
        "# 컬럼 이름 지정\n",
        "column_names = [\n",
        "    \"movie_name\", \"movie_name_English\", \"year_of_production\",\n",
        "    \"country_of_production\", \"type\", \"genre\", \"production_status\",\n",
        "    \"director\", \"production_company\"\n",
        "]\n",
        "\n",
        "data.columns = column_names\n",
        "\n",
        "# NLP를 위한 텍스트 특성 결합\n",
        "data['Text'] = data['movie_name'] + ' ' + data['production_company'] + ' ' + data['genre'] + ' ' + data['director']\n",
        "\n",
        "# 한국어 텍스트 처리를 위한 Okt 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 텍스트를 토큰화하고 원형으로 복원하는 함수 정의\n",
        "def tokenize_and_lemmatize(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text, stem=True)\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# 텍스트 데이터에 함수 적용\n",
        "data['Processed_Text'] = data['Text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# 텍스트 데이터 인코딩 및 패딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Processed_Text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Processed_Text'])\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['movie_name'])\n",
        "\n",
        "# 모델 구축\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=512, input_length=max_sequence_length))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# loss와 metrics에 mean_squared_error 정의\n",
        "model.compile(loss='mean_squared_error', optimizer=RMSprop(), metrics=['mean_squared_error'])\n",
        "\n",
        "# Early Stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(X, y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "def get_recommendations_dl(movie_title=None, genre=None, country=None, year=None):\n",
        "    if not movie_title and not genre and not country and not year:\n",
        "        return \"영화 제목, 장르, 제작 국가 또는 제작 연도 중 하나를 입력하세요.\"\n",
        "\n",
        "    if movie_title:\n",
        "        if movie_title not in data['movie_name'].values:\n",
        "            return \"영화 제목이 데이터베이스에 없습니다.\"\n",
        "\n",
        "        # 영화 제목에 해당하는 인덱스 가져오기\n",
        "        idx = data[data['movie_name'] == movie_title].index[0]\n",
        "        movie_sequence = pad_sequences(tokenizer.texts_to_sequences([data.loc[idx, 'Processed_Text']]), maxlen=max_sequence_length)\n",
        "        predictions = model.predict(movie_sequence)[0]\n",
        "\n",
        "        sim_scores = []\n",
        "        for i in range(len(predictions)):\n",
        "            sim_scores.append((i, predictions[i]))\n",
        "\n",
        "        # 유사도 순으로 정렬\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 상위 5개 추천\n",
        "        sim_scores = sim_scores[:5]\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        recommendations = data.iloc[movie_indices]\n",
        "\n",
        "        # 시각화를 위한 유사도 점수 포함\n",
        "        similarity_scores = [i[1] for i in sim_scores]\n",
        "        return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "    else:\n",
        "        # 필터링된 영화에서 상위 5개 선택\n",
        "        recommendations = data.head(5)\n",
        "        similarity_scores = [1] * len(recommendations)  # 필터링된 결과에서는 유사도 점수를 1로 설정\n",
        "\n",
        "    recommendations = recommendations.copy()\n",
        "    recommendations.loc[:, 'year_of_production'] = recommendations['year_of_production'].astype(int)\n",
        "    return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "# 사용자 입력을 통한 영화 제목, 장르, 제작 국가, 제작 연도 입력\n",
        "user_input = input(\"영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): \").strip()\n",
        "\n",
        "# 입력된 값을 쉼표로 구분하여 리스트로 변환\n",
        "inputs = [i.strip() if i.strip() else None for i in user_input.split(',')]\n",
        "inputs = inputs + [None] * (4 - len(inputs))  # 입력된 값의 개수가 4개가 되도록 보충\n",
        "movie_title, genre, country, year = inputs\n",
        "\n",
        "recommended_movies, similarity_scores = get_recommendations_dl(movie_title, genre, country, year)\n",
        "print(\"추천된 영화들:\\n\", recommended_movies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNost4dsFabz",
        "outputId": "121002aa-8b78-48aa-9514-08f39806ff02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "83/83 [==============================] - 26s 139ms/step - loss: 3570282.5000 - mean_squared_error: 3570282.5000 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 2/100\n",
            "83/83 [==============================] - 3s 38ms/step - loss: 3570282.0000 - mean_squared_error: 3570282.0000 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 3/100\n",
            "83/83 [==============================] - 3s 35ms/step - loss: 3570281.7500 - mean_squared_error: 3570281.7500 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 4/100\n",
            "83/83 [==============================] - 2s 29ms/step - loss: 3570282.0000 - mean_squared_error: 3570282.0000 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 5/100\n",
            "83/83 [==============================] - 4s 50ms/step - loss: 3570281.7500 - mean_squared_error: 3570281.7500 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 6/100\n",
            "83/83 [==============================] - 3s 31ms/step - loss: 3570282.5000 - mean_squared_error: 3570282.5000 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 7/100\n",
            "83/83 [==============================] - 2s 23ms/step - loss: 3570281.7500 - mean_squared_error: 3570281.7500 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 8/100\n",
            "83/83 [==============================] - 3s 32ms/step - loss: 3570282.0000 - mean_squared_error: 3570282.0000 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 9/100\n",
            "83/83 [==============================] - 3s 31ms/step - loss: 3570281.7500 - mean_squared_error: 3570281.7500 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 10/100\n",
            "83/83 [==============================] - 3s 34ms/step - loss: 3570282.0000 - mean_squared_error: 3570282.0000 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "Epoch 11/100\n",
            "83/83 [==============================] - 2s 24ms/step - loss: 3570281.7500 - mean_squared_error: 3570281.7500 - val_loss: 3823496.7500 - val_mean_squared_error: 3823496.7500\n",
            "영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): 아이언맨 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x791efb2af7f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "추천된 영화들:\n",
            " [['익스펜더블 에셋', '액션,전쟁', '미국', 2016.0], ['극장판 숲의 요정 페어리루 ~크리스마스의 기적: 마법의 날개~', '애니메이션', '일본', 2017.0], ['주피터 어센딩', 'SF,액션,어드벤처', '미국', 2014.0], ['그란 투리스모', '액션', '미국', 2023.0], ['오늘', '드라마', '한국', 2011.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 파일 불러오기\n",
        "file_path = '/content/drive/MyDrive/movie12.xls'\n",
        "data = pd.read_excel(file_path, engine='xlrd', header=None)\n",
        "\n",
        "# 컬럼 이름 지정\n",
        "column_names = [\n",
        "    \"movie_name\", \"movie_name_English\", \"year_of_production\",\n",
        "    \"country_of_production\", \"type\", \"genre\", \"production_status\",\n",
        "    \"director\", \"production_company\"\n",
        "]\n",
        "\n",
        "data.columns = column_names\n",
        "\n",
        "# NLP를 위한 텍스트 특성 결합\n",
        "data['Text'] = data['movie_name'] + ' ' + data['production_company'] + ' ' + data['genre'] + ' ' + data['director']\n",
        "\n",
        "# 한국어 텍스트 처리를 위한 Okt 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 텍스트를 토큰화하고 원형으로 복원하는 함수 정의\n",
        "def tokenize_and_lemmatize(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text, stem=True)\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# 텍스트 데이터에 함수 적용\n",
        "data['Processed_Text'] = data['Text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# 텍스트 데이터 인코딩 및 패딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Processed_Text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Processed_Text'])\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['movie_name'])\n",
        "\n",
        "# 모델 구축\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "def get_recommendations_dl(movie_title=None, genre=None, country=None, year=None):\n",
        "    if not movie_title and not genre and not country and not year:\n",
        "        return \"영화 제목, 장르, 제작 국가 또는 제작 연도 중 하나를 입력하세요.\"\n",
        "\n",
        "    if movie_title:\n",
        "        if movie_title not in data['movie_name'].values:\n",
        "            return \"영화 제목이 데이터베이스에 없습니다.\"\n",
        "\n",
        "        # 영화 제목에 해당하는 인덱스 가져오기\n",
        "        idx = data[data['movie_name'] == movie_title].index[0]\n",
        "        movie_sequence = pad_sequences(tokenizer.texts_to_sequences([data.loc[idx, 'Processed_Text']]), maxlen=max_sequence_length)\n",
        "        predictions = model.predict(movie_sequence)[0]\n",
        "\n",
        "        sim_scores = []\n",
        "        for i in range(len(predictions)):\n",
        "            sim_scores.append((i, predictions[i]))\n",
        "\n",
        "        # 유사도 순으로 정렬\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 상위 5개 추천\n",
        "        sim_scores = sim_scores[:5]\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        recommendations = data.iloc[movie_indices]\n",
        "\n",
        "        # 시각화를 위한 유사도 점수 포함\n",
        "        similarity_scores = [i[1] for i in sim_scores]\n",
        "        return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "    else:\n",
        "        # 필터링된 영화에서 상위 5개 선택\n",
        "        recommendations = data.head(5)\n",
        "        similarity_scores = [1] * len(recommendations)  # 필터링된 결과에서는 유사도 점수를 1로 설정\n",
        "\n",
        "    recommendations = recommendations.copy()\n",
        "    recommendations.loc[:, 'year_of_production'] = recommendations['year_of_production'].astype(int)\n",
        "    return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "# 사용자 입력을 통한 영화 제목, 장르, 제작 국가, 제작 연도 입력\n",
        "user_input = input(\"영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): \").strip()\n",
        "\n",
        "# 입력된 값을 쉼표로 구분하여 리스트로 변환\n",
        "inputs = [i.strip() if i.strip() else None for i in user_input.split(',')]\n",
        "inputs = inputs + [None] * (4 - len(inputs))  # 입력된 값의 개수가 4개가 되도록 보충\n",
        "movie_title, genre, country, year = inputs\n",
        "\n",
        "recommended_movies, similarity_scores = get_recommendations_dl(movie_title, genre, country, year)\n",
        "print(\"추천된 영화들:\\n\", recommended_movies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "RRVjX-RhLrvL",
        "outputId": "8f1a815c-01a7-4e2f-d048-9462f6929fde"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3294) are incompatible\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4a2623b2d680>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_recommendations_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3294) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from konlpy.tag import Okt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 파일 불러오기\n",
        "file_path = '/content/drive/MyDrive/movie12.xls'\n",
        "data = pd.read_excel(file_path, engine='xlrd', header=None)\n",
        "\n",
        "# 컬럼 이름 지정\n",
        "column_names = [\n",
        "    \"movie_name\", \"movie_name_English\", \"year_of_production\",\n",
        "    \"country_of_production\", \"type\", \"genre\", \"production_status\",\n",
        "    \"director\", \"production_company\"\n",
        "]\n",
        "\n",
        "data.columns = column_names\n",
        "\n",
        "# NLP를 위한 텍스트 특성 결합\n",
        "data['Text'] = data['movie_name'] + ' ' + data['production_company'] + ' ' + data['genre'] + ' ' + data['director']\n",
        "\n",
        "# 한국어 텍스트 처리를 위한 Okt 초기화\n",
        "okt = Okt()\n",
        "\n",
        "# 텍스트를 토큰화하고 원형으로 복원하는 함수 정의\n",
        "def tokenize_and_lemmatize(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = okt.morphs(text, stem=True)\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# 텍스트 데이터에 함수 적용\n",
        "data['Processed_Text'] = data['Text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# 텍스트 데이터 인코딩 및 패딩\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['Processed_Text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Processed_Text'])\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data['movie_name'])\n",
        "\n",
        "# 레이블을 원-핫 인코딩으로 변환\n",
        "y = to_categorical(y)\n",
        "\n",
        "# 모델 구축\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length))\n",
        "model.add(Dropout(0.2))  # Dropout 비율 낮춤\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.2))  # Dropout 비율 낮춤\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(Dropout(0.2))  # Dropout 비율 낮춤\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# 예측 함수를 tf.function으로 정의\n",
        "@tf.function\n",
        "def predict(model, sequence):\n",
        "    return model(sequence, training=False)\n",
        "\n",
        "def get_recommendations_dl(movie_title=None, genre=None, country=None, year=None):\n",
        "    if not movie_title and not genre and not country and not year:\n",
        "        return \"영화 제목, 장르, 제작 국가 또는 제작 연도 중 하나를 입력하세요.\"\n",
        "\n",
        "    if movie_title:\n",
        "        if movie_title not in data['movie_name'].values:\n",
        "            return \"영화 제목이 데이터베이스에 없습니다.\"\n",
        "\n",
        "        # 영화 제목에 해당하는 인덱스 가져오기\n",
        "        idx = data[data['movie_name'] == movie_title].index[0]\n",
        "        movie_sequence = pad_sequences(tokenizer.texts_to_sequences([data.loc[idx, 'Processed_Text']]), maxlen=max_sequence_length)\n",
        "        movie_sequence = tf.convert_to_tensor(movie_sequence)  # 텐서로 변환\n",
        "        predictions = predict(model, movie_sequence)[0].numpy()\n",
        "\n",
        "        sim_scores = []\n",
        "        for i in range(len(predictions)):\n",
        "            sim_scores.append((i, predictions[i]))\n",
        "\n",
        "        # 유사도 순으로 정렬\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # 상위 5개 추천\n",
        "        sim_scores = sim_scores[:5]\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "        recommendations = data.iloc[movie_indices]\n",
        "\n",
        "        # 시각화를 위한 유사도 점수 포함\n",
        "        similarity_scores = [i[1] for i in sim_scores]\n",
        "        return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "    else:\n",
        "        # 필터링된 영화에서 상위 5개 선택\n",
        "        recommendations = data.head(5)\n",
        "        similarity_scores = [1] * len(recommendations)  # 필터링된 결과에서는 유사도 점수를 1로 설정\n",
        "\n",
        "    recommendations = recommendations.copy()\n",
        "    recommendations.loc[:, 'year_of_production'] = recommendations['year_of_production'].astype(int)\n",
        "    return recommendations[['movie_name', 'genre', 'country_of_production', 'year_of_production']].values.tolist(), similarity_scores\n",
        "\n",
        "# 사용자 입력을 통한 영화 제목, 장르, 제작 국가, 제작 연도 입력\n",
        "user_input = input(\"영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): \").strip()\n",
        "\n",
        "# 입력된 값을 쉼표로 구분하여 리스트로 변환\n",
        "inputs = [i.strip() if i.strip() else None for i in user_input.split(',')]\n",
        "inputs = inputs + [None] * (4 - len(inputs))  # 입력된 값의 개수가 4개가 되도록 보충\n",
        "movie_title, genre, country, year = inputs\n",
        "\n",
        "recommended_movies, similarity_scores = get_recommendations_dl(movie_title, genre, country, year)\n",
        "print(\"추천된 영화들:\\n\", recommended_movies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRbeC4MGPhlz",
        "outputId": "6c6ffc66-d7d1-4d13-d04a-b94124488163"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "83/83 [==============================] - 18s 103ms/step - loss: 8.3836 - accuracy: 0.0000e+00 - val_loss: 8.2833 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "83/83 [==============================] - 5s 59ms/step - loss: 8.1744 - accuracy: 0.0000e+00 - val_loss: 8.2446 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/30\n",
            "83/83 [==============================] - 2s 29ms/step - loss: 8.0957 - accuracy: 0.0000e+00 - val_loss: 8.3112 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/30\n",
            "83/83 [==============================] - 2s 22ms/step - loss: 7.9682 - accuracy: 0.0000e+00 - val_loss: 8.4868 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/30\n",
            "83/83 [==============================] - 2s 27ms/step - loss: 7.8313 - accuracy: 3.7722e-04 - val_loss: 8.6646 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/30\n",
            "83/83 [==============================] - 1s 16ms/step - loss: 7.7593 - accuracy: 0.0011 - val_loss: 8.9100 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/30\n",
            "83/83 [==============================] - 1s 17ms/step - loss: 7.7311 - accuracy: 3.7722e-04 - val_loss: 9.0417 - val_accuracy: 0.0000e+00\n",
            "영화 제목, 장르, 제작 국가, 제작 연도를 쉼표로 구분하여 입력하세요 (예: 제목,장르,국가,연도): 아이언맨\n",
            "추천된 영화들:\n",
            " [['버스납치 리턴즈', '코미디,드라마', '일본', 2017.0], ['드래곤볼 에볼루션', '액션,어드벤처,SF,가족', '미국', 2009.0], ['모테키: 모태솔로 탈출기', '코미디,멜로/로맨스', '일본', 2011.0], ['빌리어네어 보이즈클럽', '드라마,범죄', '미국', 2016.0], ['로또쉐어', '스릴러,액션,범죄', '한국', 2019.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hug385RAPvnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1q0ZUaOFBwcwiUpehO8rrRcmlGN9DNmZ4",
      "authorship_tag": "ABX9TyNbR9yoa7NUbkhDN9VG42Sa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}