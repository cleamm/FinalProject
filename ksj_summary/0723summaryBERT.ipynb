{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **사용자가 텍스트를 입력하면 요약해 주는 역할관련 논문 및 사이트**\n",
        "# **BERTSUM 모델**\n",
        "**추출적 요약**\n",
        "- BERTSUM 모델은 BERT 아키텍처를 확장하여 각 문장을 요약에 포함할지 여부를 결정하는 분류 레이어를 추가합니다. 이 모델은 문서 수준의 문맥을 효과적으로 캡처하여 CNN/DailyMail과 같은 데이터셋에서 최첨단 성능을 보여주고 있음\n",
        " - https://ar5iv.labs.arxiv.org/html/1908.08345\n",
        " - https://paperswithcode.com/paper/fine-tune-bert-for-extractive-summarization\n",
        "\n",
        "**생성적 요약**\n",
        "- 생성적 요약의 경우 BERTSUM은 사전 훈련된 BERT 인코더와 무작위로 초기화된 변환기 디코더를 조합하여 사용\n",
        "- 이를 통해 원본 텍스트에서 직접 가져오지 않은 문구로 요약을 생성 가능\n",
        " - https://ar5iv.labs.arxiv.org/html/1908.08345\n",
        " - https://deeplearninganalytics.org/text-summarization/\n",
        "\n",
        "# **관련 논문 및 리소스**\n",
        "**1. \"Text Summarization with Pretrained Encoders\"**\n",
        "- 이 논문은 사전 훈련된 인코더를 사용하여 텍스트 요약을 수행하는 방법을 설명\n",
        "- BERT를 사용한 요약의 구체적인 방법론과 실험 결과\n",
        " - https://arxiv.org/abs/1908.08345\n",
        "**2. \"Recent Progress on Text Summarisation Based on BERT and GPT\"**\n",
        "- 이 논문은 BERT 및 GPT 기반의 텍스트 요약의 최근 발전을 설명\n",
        "- BERTSUM 모델의 성능과 다양한 데이터셋에 대한 적용 사례를 포함\n",
        " - https://link.springer.com/article/10.1007/s00138-021-01116-2\n",
        "**3.\"Performance Study on Extractive Text Summarization Using BERT Models\"**\n",
        "- 이 연구는 BERT 모델의 다양한 변종을 사용한 추출적 텍스트 요약 성능을 실험하고 평가\n",
        "- 이 논문에서는 \"SqueezeBERTSum\"이라는 경량화된 요약 모델\n",
        " - https://www.mdpi.com/2078-2489/13/2/67"
      ],
      "metadata": {
        "id": "VimqUmMlftVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **데이터셋**\n",
        "- AI허브\n",
        " - 문서요약 텍스트\n",
        "   - https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=97\n",
        " - 요약문 및 레포트 생성 데이터\n",
        "   - https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=582\n",
        " - 한국어 대화 요약\n",
        "   - https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=117\n",
        " - 도서자료 요약\n",
        "   - https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=93\n",
        "\n",
        "# **그외 사이트의 데이터**\n",
        " - Korean NLP Tutorial\n",
        "   - https://github.com/Seokii/Korean_NLP_Tutorial\n",
        "\n",
        "\n",
        "\n",
        "# **관련 및 참고 코드 사이트**\n",
        "- Study ML\n",
        " - https://seokii.tistory.com/145\n",
        "\n",
        "\n",
        "# **데이터 및 코드**\n",
        " - 한국어 문서 요약 AI 경진대회\n",
        "  - https://dacon.io/competitions/official/235673/data"
      ],
      "metadata": {
        "id": "efc4PCgDh51t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\"4. 데이터셋 로드 및 전처리\"에서 UTF-8관련 오류가 나와서 찾고 있어서 전체적으로 테스트 못하였습니다**\n",
        "**1. 테스트로 한 파일**\n",
        " - valid_original 1.json\n",
        " - train_original 1.json\n",
        "\n",
        "**2. 오류 내용**\n",
        "\n",
        " <!-- -UnicodeDecodeError                        Traceback (most recent call last)\n",
        "<ipython-input-17-3da4d3478975> in <cell line: 23>()\n",
        "     21\n",
        "     22 # JSON 데이터셋 로드\n",
        "---> 23 df = pd.read_json(DATA_TRAIN_PATH, encoding='utf-8')\n",
        "     24 df = df.dropna()\n",
        "     25 test_df = pd.read_json(DATA_TEST_PATH, encoding='utf-8')\n",
        "\n",
        "3 frames\n",
        "/usr/lib/python3.10/codecs.py in decode(self, input, final)\n",
        "    320         # decode input (taking the buffer into account)\n",
        "    321         data = self.buffer + input\n",
        "--> 322         (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
        "    323         # keep undecoded input until the next call\n",
        "    324         self.buffer = data[consumed:]\n",
        "\n",
        "UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 5242878-5242879: unexpected end of data -->\n",
        "\n",
        "**3. 오류 원인을 찾고 해결을 찾고 있음**"
      ],
      "metadata": {
        "id": "nQTBh40UdeHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 필요한 라이브러리 설치**"
      ],
      "metadata": {
        "id": "jGRSYEJD247a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip install transformers torch sklearn pandas\n",
        "!pip install pyngrok\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!pip install pytorch_lightning\n",
        "!pip install torchmetrics\n",
        "!pip install pytorch-lightning\n",
        "!pip install kss\n",
        "!pip install pytorch-lightning\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "MZIGYH9VmGGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "601d6875-58b7-43d0-cba1-122fd6edfb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.10/dist-packages (0.11.10)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from matplotlib-venn) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.16.0)\n",
            "E: Package 'libfluidsynth1' has no installation candidate\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.4.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.11.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.32.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch_lightning) (12.5.82)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch_lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.4.0.post0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.4.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.11.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.32.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch-lightning) (12.5.82)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
            "Collecting kss\n",
            "  Downloading kss-6.0.4.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.3)\n",
            "Collecting jamo (from kss)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Collecting hangul-jamo (from kss)\n",
            "  Downloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
            "Collecting tossi (from kss)\n",
            "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting distance (from kss)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml==6.0 (from kss)\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidecode (from kss)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmudict (from kss)\n",
            "  Downloading cmudict-1.0.26-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting koparadigm (from kss)\n",
            "  Downloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kollocate (from kss)\n",
            "  Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4 (from kss)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kss) (1.25.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from kss) (7.4.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kss) (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->kss) (4.12.3)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (7.1.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (6.4.0)\n",
            "Collecting whoosh (from kollocate->kss)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd==1.2.0 (from koparadigm->kss)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (17.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (2023.12.25)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.1)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (0.23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (1.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict->kss) (3.18.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->kss) (2.5)\n",
            "Building wheels for collected packages: kss, distance, pecab, tossi\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-6.0.4-cp310-cp310-linux_x86_64.whl size=1446486 sha256=c42c9e427db891b429bae9a5e6522f7e6d4393e488824ea2be3e9ffe9d3fbe43\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/70/d5/c9308346829b1eb9e7267d74696919d2453aee6ce350f98b3b\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=a574bc29c570fc6448b3fb2c8c12c38824ddc2eaede6e8639fde636f33711c26\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=15968d0e5082edb337ea9deaf2c40ad86135b3d33b2586079c2dd5a9c99ef536\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "  Building wheel for tossi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12130 sha256=d34a5f943ef43b6cad29103b44d0845175dd4a6e340daf80526b65aab70be9a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/18/60/1094a6fe93c8063efcd3e6700d09328216682e495a3c51af9f\n",
            "Successfully built kss distance pecab tossi\n",
            "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, tossi, pyyaml, kollocate, pecab, koparadigm, cmudict, bs4, kss\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "Successfully installed bs4-0.0.2 cmudict-1.0.26 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.4 pecab-1.0.8 pyyaml-6.0 tossi-0.3.1 unidecode-1.3.8 whoosh-2.7.4 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              },
              "id": "d162a6f35c704c21aa74f2ea17a117fa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 필요한 라이브러리 임포트**"
      ],
      "metadata": {
        "id": "KkKPBf2C5gpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.nn.init import xavier_uniform_\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.functional import accuracy, f1_score, auroc\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "import kss\n",
        "\n",
        "# Colab에서 matplotlib 설정\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ],
      "metadata": {
        "id": "g29lt5K-5jz4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 시드 및 하이퍼파라미터 설정**"
      ],
      "metadata": {
        "id": "25xc2Y9xZJB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "pl.seed_everything(RANDOM_SEED)\n",
        "\n",
        "MAX_TOKEN_COUNT = 512\n",
        "N_EPOCHS = 10\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOCOlSz0ZP90",
        "outputId": "b5e84825-02bc-4e2d-9ec8-99ddeb5a4dc5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 데이터셋 로드 및 전처리**"
      ],
      "metadata": {
        "id": "SgOHWYmAYzJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data와 test_data의 첫 번째 항목 구조 확인\n",
        "print(json.dumps(train_data[0], indent=4, ensure_ascii=False))\n",
        "print(json.dumps(test_data[0], indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOOciqaso0bG",
        "outputId": "50216760-09a4-4839-cb0b-54c5437c708b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. 데이터셋 로드 및 전처리\n",
        "DATA_TRAIN_PATH = '/content/train_original.json'\n",
        "DATA_TEST_PATH = '/content/vaild_original.json'\n",
        "\n",
        "# JSON 데이터셋 로드\n",
        "df = pd.read_json(DATA_TRAIN_PATH)\n",
        "df = df.dropna()\n",
        "test_df = pd.read_json(DATA_TEST_PATH)\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "# 학습 데이터와 검증 데이터로 분할\n",
        "train_df, val_df = train_test_split(df, test_size=0.05)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "# 데이터 다운사이즈 (테스트용)\n",
        "downsize = 2000\n",
        "train_df = train_df[:downsize]\n",
        "test_df = test_df[:downsize//10]\n",
        "val_df = val_df[:downsize//10]\n",
        "\n",
        "# 데이터 전처리 함수\n",
        "def preprocess_data(data):\n",
        "    outs = []\n",
        "    for doc in data['documents']:\n",
        "        line = []\n",
        "        line.append(doc['media_name'])\n",
        "        line.append(doc['id'])\n",
        "        para = []\n",
        "        for sent in doc['text']:\n",
        "            for s in sent:\n",
        "                para.append(s['sentence'])\n",
        "        line.append(para)\n",
        "        line.append(doc['abstractive'][0])\n",
        "        line.append(doc['extractive'])\n",
        "        a = doc['extractive']\n",
        "        if a[0] == None or a[1] == None or a[2] == None:\n",
        "            continue\n",
        "        outs.append(line)\n",
        "    outs_df = pd.DataFrame(outs)\n",
        "    outs_df.columns = ['media', 'id', 'article_original', 'abstractive', 'extractive']\n",
        "    return outs_df\n",
        "\n",
        "# 전처리된 데이터\n",
        "train_df = preprocess_data(train_df)\n",
        "test_df = preprocess_data(test_df)\n",
        "val_df = preprocess_data(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Iw6cWD7MY363",
        "outputId": "3178b1fa-b8bd-4c10-c104-f6f86ecd5f7b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-98bd2f73bf4b>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# 전처리된 데이터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-98bd2f73bf4b>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'media_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mpara\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. BERT 모델과 토크나이저 로드**"
      ],
      "metadata": {
        "id": "9ajbpBZQZccI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
      ],
      "metadata": {
        "id": "rPOvn924ZeJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. 데이터셋 클래스 정의**"
      ],
      "metadata": {
        "id": "viKOqyF7ZhEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame, tokenizer: BertTokenizer, max_token_len: int = 512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]\n",
        "        tokenlist = []\n",
        "        for sent in data_row.article_original:\n",
        "            tokenlist.append(tokenizer(\n",
        "                text = sent,\n",
        "                add_special_tokens = True))\n",
        "\n",
        "        src = []  # 토크나이징 된 전체 문단\n",
        "        labels = []  # 요약문에 해당하면 1, 아니면 0으로 문장수 만큼 생성\n",
        "        segs = []  # 각 토큰에 대해 홀수번째 문장이면 0, 짝수번째 문장이면 1을 매핑\n",
        "        clss = []  # [CLS] 토큰의 포지션값을 지정\n",
        "\n",
        "        odd = 0\n",
        "        for tkns in tokenlist:\n",
        "            if odd > 1 : odd = 0\n",
        "            clss = clss + [len(src)]\n",
        "            src = src + tkns['input_ids']\n",
        "            segs = segs + [odd] * len(tkns['input_ids'])\n",
        "            if tokenlist.index(tkns) in data_row.extractive:\n",
        "                labels = labels + [1]\n",
        "            else:\n",
        "                labels = labels + [0]\n",
        "            odd += 1\n",
        "\n",
        "            # 문장 길이 제한 (truncation)\n",
        "            if len(src) == MAX_TOKEN_COUNT:\n",
        "                break\n",
        "            elif len(src) > MAX_TOKEN_COUNT:\n",
        "                src = src[:self.max_token_len - 1] + [src[-1]]\n",
        "                segs = segs[:self.max_token_len]\n",
        "                break\n",
        "\n",
        "        # 패딩 (padding)\n",
        "        if len(src) < MAX_TOKEN_COUNT:\n",
        "            src = src + [0]*(self.max_token_len - len(src))\n",
        "            segs = segs + [0]*(self.max_token_len - len(segs))\n",
        "\n",
        "        if len(clss) < MAX_TOKEN_COUNT:\n",
        "            clss = clss + [-1]*(self.max_token_len - len(clss))\n",
        "        if len(labels) < MAX_TOKEN_COUNT:\n",
        "            labels = labels + [0]*(self.max_token_len - len(labels))\n",
        "\n",
        "        return dict(\n",
        "            src = torch.tensor(src),\n",
        "            segs = torch.tensor(segs),\n",
        "            clss = torch.tensor(clss),\n",
        "            labels= torch.FloatTensor(labels)\n",
        "        )"
      ],
      "metadata": {
        "id": "J7h-K45TZl3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 데이터 모듈 정의**"
      ],
      "metadata": {
        "id": "w9Gpdy0fZvVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_df, test_df, val_df, tokenizer, batch_size=1, max_token_len=512):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.val_df = val_df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_token_len = max_token_len\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = SummDataset(self.train_df, self.tokenizer, self.max_token_len)\n",
        "        self.test_dataset = SummDataset(self.test_df, self.tokenizer, self.max_token_len)\n",
        "        self.val_dataset = SummDataset(self.val_df, self.tokenizer, self.max_token_len)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=0)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0)\n",
        "\n",
        "data_module = SummDataModule(train_df, test_df, val_df, tokenizer, batch_size=BATCH_SIZE, max_token_len=MAX_TOKEN_COUNT)\n"
      ],
      "metadata": {
        "id": "EUq_xqpTZxam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. 모델 클래스 정의**"
      ],
      "metadata": {
        "id": "DQh7MPoCZ4UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 클래스 정의 (PositionalEncoding)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dropout, dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.dim = dim\n",
        "\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) * -(math.log(10000.0) / dim)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, emb, step=None):\n",
        "        emb = emb * math.sqrt(self.dim)\n",
        "        if (step):\n",
        "            emb = emb + self.pe[:, step][:, None, :]\n",
        "        else:\n",
        "            emb = emb + self.pe[:, :emb.size(1)]\n",
        "        emb = self.dropout(emb)\n",
        "        return emb\n",
        "\n",
        "    def get_emb(self, emb):\n",
        "        return self.pe[:, :emb.size(1)]\n",
        "\n",
        "# 모델 클래스 정의 (TransformerEncoderLayer)\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadedAttention(heads, d_model, dropout=dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, iter, query, inputs, mask):\n",
        "        if (iter != 0):\n",
        "            input_norm = self.layer_norm(inputs)\n",
        "        else:\n",
        "            input_norm = inputs\n",
        "\n",
        "        mask = mask.unsqueeze(1)\n",
        "        context = self.self_attn(input_norm, input_norm, input_norm, mask=mask)\n",
        "        out = self.dropout(context) + inputs\n",
        "        return self.feed_forward(out)\n",
        "\n",
        "# 모델 클래스 정의 (ExtTransformerEncoder)\n",
        "class ExtTransformerEncoder(nn.Module):\n",
        "    def __init__(self, hidden_size=768, d_ff=2048, heads=8, dropout=0.2, num_inter_layers=2):\n",
        "        super(ExtTransformerEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_inter_layers = num_inter_layers\n",
        "        self.pos_emb = PositionalEncoding(dropout, hidden_size)\n",
        "        self.transformer_inter = nn.ModuleList([TransformerEncoderLayer(hidden_size, heads, d_ff, dropout) for _ in range(num_inter_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "        self.wo = nn.Linear(hidden_size, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, top_vecs, mask):\n",
        "        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n",
        "        pos_emb = self.pos_emb.pe[:, :n_sents]\n",
        "        x = top_vecs * mask[:, :, None].float()\n",
        "        x = x + pos_emb\n",
        "\n",
        "        for i in range(self.num_inter_layers):\n",
        "            x = self.transformer_inter[i](i, x, x, ~mask)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        sent_scores = self.sigmoid(self.wo(x))\n",
        "        sent_scores = sent_scores.squeeze(-1) * mask.float()\n",
        "\n",
        "        return sent_scores\n",
        "\n",
        "# 모델 클래스 정의 (PositionwiseFeedForward)\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def gelu(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "    def forward(self, x):\n",
        "        inter = self.dropout_1(self.gelu(self.w_1(self.layer_norm(x))))\n",
        "        output = self.dropout_2(self.w_2(inter))\n",
        "        return output + x\n",
        "\n",
        "# 모델 클래스 정의 (MultiHeadedAttention)\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n",
        "        assert model_dim % head_count == 0\n",
        "        self.dim_per_head = model_dim // head_count\n",
        "        self.model_dim = model_dim\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.head_count = head_count\n",
        "\n",
        "        self.linear_keys = nn.Linear(model_dim, head_count * self.dim_per_head)\n",
        "        self.linear_values = nn.Linear(model_dim, head_count * self.dim_per_head)\n",
        "        self.linear_query = nn.Linear(model_dim, head_count * self.dim_per_head)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.use_final_linear = use_final_linear\n",
        "        if (self.use_final_linear):\n",
        "            self.final_linear = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def forward(self, key, value, query, mask=None, layer_cache=None, type=None, predefined_graph_1=None):\n",
        "        batch_size = key.size(0)\n",
        "        dim_per_head = self.dim_per_head\n",
        "        head_count = self.head_count\n",
        "        key_len = key.size(1)\n",
        "        query_len = query.size(1)\n",
        "\n",
        "        def shape(x):\n",
        "            return x.view(batch_size, -1, head_count, dim_per_head).transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            return x.transpose(1, 2).contiguous().view(batch_size, -1, head_count * dim_per_head)\n",
        "\n",
        "        if layer_cache is not None:\n",
        "            if type == \"self\":\n",
        "                query, key, value = self.linear_query(query), self.linear_keys(query), self.linear_values(query)\n",
        "                key = shape(key)\n",
        "                value = shape(value)\n",
        "                if layer_cache is not None:\n",
        "                    device = key.device\n",
        "                    if layer_cache[\"self_keys\"] is not None:\n",
        "                        key = torch.cat((layer_cache[\"self_keys\"].to(device), key), dim=2)\n",
        "                    if layer_cache[\"self_values\"] is not None:\n",
        "                        value = torch.cat((layer_cache[\"self_values\"].to(device), value), dim=2)\n",
        "                    layer_cache[\"self_keys\"] = key\n",
        "                    layer_cache[\"self_values\"] = value\n",
        "            elif type == \"context\":\n",
        "                query = self.linear_query(query)\n",
        "                if layer_cache is not None:\n",
        "                    if layer_cache[\"memory_keys\"] is None:\n",
        "                        key, value = self.linear_keys(key), self.linear_values(value)\n",
        "                        key = shape(key)\n",
        "                        value = shape(value)\n",
        "                    else:\n",
        "                        key, value = layer_cache[\"memory_keys\"], layer_cache[\"memory_values\"]\n",
        "                    layer_cache[\"memory_keys\"] = key\n",
        "                    layer_cache[\"memory_values\"] = value\n",
        "                else:\n",
        "                    key, value = self.linear_keys(key), self.linear_values(value)\n",
        "                    key = shape(key)\n",
        "                    value = shape(value)\n",
        "        else:\n",
        "            key = self.linear_keys(key)\n",
        "            value = self.linear_values(value)\n",
        "            query = self.linear_query(query)\n",
        "            key = shape(key)\n",
        "            value = shape(value)\n",
        "\n",
        "        query = shape(query)\n",
        "        key_len = key.size(2)\n",
        "        query_len = query.size(2)\n",
        "        query = query / math.sqrt(dim_per_head)\n",
        "        scores = torch.matmul(query, key.transpose(2, 3))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).expand_as(scores)\n",
        "            scores = scores.masked_fill(mask, -1e18)\n",
        "\n",
        "        attn = self.softmax(scores)\n",
        "\n",
        "        if (not predefined_graph_1 is None):\n",
        "            attn_masked = attn[:, -1] * predefined_graph_1\n",
        "            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n",
        "            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n",
        "\n",
        "        drop_attn = self.dropout(attn)\n",
        "        if (self.use_final_linear):\n",
        "            context = unshape(torch.matmul(drop_attn, value))\n",
        "            output = self.final_linear(context)\n",
        "            return output\n",
        "        else:\n",
        "            context = torch.matmul(drop_attn, value)\n",
        "            return context\n",
        "\n",
        "# 모델 클래스 정의 (Summarizer)\n",
        "class Summarizer(pl.LightningModule):\n",
        "    def __init__(self, n_training_steps=None, n_warmup_steps=None):\n",
        "        super().__init__()\n",
        "        self.max_pos = 512\n",
        "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
        "        self.ext_layer = ExtTransformerEncoder()\n",
        "        self.n_training_steps = n_training_steps\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.loss = nn.BCELoss(reduction='none')\n",
        "\n",
        "        for p in self.ext_layer.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, segs, clss, labels=None):\n",
        "        mask_src = ~(src == 0)\n",
        "        mask_cls = ~(clss == -1)\n",
        "        top_vec = self.bert(src, token_type_ids=segs, attention_mask=mask_src)\n",
        "        top_vec = top_vec.last_hidden_state\n",
        "        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
        "        sents_vec = sents_vec * mask_cls[:, :, None].float()\n",
        "        sent_scores = self.ext_layer(sents_vec, mask_cls).squeeze(-1)\n",
        "\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.loss(sent_scores, labels)\n",
        "            loss = (loss * mask_cls.float()).sum() / len(labels)\n",
        "\n",
        "        return loss, sent_scores\n",
        "\n",
        "    def step(self, batch):\n",
        "        src = batch['src']\n",
        "        if len(batch['labels']) > 0:\n",
        "            labels = batch['labels']\n",
        "        else:\n",
        "            labels = None\n",
        "        segs = batch['segs']\n",
        "        clss = batch['clss']\n",
        "        loss, sent_scores = self(src, segs, clss, labels)\n",
        "        return loss, sent_scores, labels\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, sent_scores, labels = self.step(batch)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, sent_scores, labels = self.step(batch)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, sent_scores, labels = self.step(batch)\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "        return {\"loss\": loss, \"predictions\": sent_scores, \"labels\": labels}\n",
        "\n",
        "    def acc_loss(self, outputs):\n",
        "        total_loss = 0\n",
        "        hit_cnt = 0\n",
        "        for outp in outputs:\n",
        "            labels = outp['labels'].cpu()\n",
        "            predictions, idxs = outp['predictions'].cpu().sort()\n",
        "            loss = outp['loss'].cpu()\n",
        "            for label, idx in zip(labels, idxs):\n",
        "                for i in range(1,3):\n",
        "                    if label[idx[-i-1]] == 1:\n",
        "                        hit_cnt += 1\n",
        "            total_loss += loss\n",
        "        avg_loss = total_loss / len(outputs)\n",
        "        acc = hit_cnt / (3*len(outputs)*len(labels))\n",
        "        return acc, avg_loss\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        acc, avg_loss = self.acc_loss(outputs)\n",
        "        print('acc:', acc, 'avg_loss:', avg_loss)\n",
        "        self.log('avg_train_loss', avg_loss, prog_bar=True, logger=True)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        acc, avg_loss = self.acc_loss(outputs)\n",
        "        print('val_acc:', acc, 'avg_val_loss:', avg_loss)\n",
        "        self.log('avg_val_loss', avg_loss, prog_bar=True, logger=True)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        acc, avg_loss = self.acc_loss(outputs)\n",
        "        print('test_acc:', acc, 'avg_test_loss:', avg_loss)\n",
        "        self.log('avg_test_loss', avg_loss, prog_bar=True, logger=True)\n",
        "        return\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
        "        steps_per_epoch=len(train_df) // BATCH_SIZE\n",
        "        total_training_steps = steps_per_epoch * N_EPOCHS\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=steps_per_epoch,\n",
        "            num_training_steps=total_training_steps\n",
        "        )\n",
        "        return dict(\n",
        "            optimizer=optimizer,\n",
        "            lr_scheduler=dict(\n",
        "                scheduler=scheduler,\n",
        "                interval='step'\n",
        "            )\n",
        "        )\n",
        "\n",
        "model = Summarizer()"
      ],
      "metadata": {
        "id": "Qv_XFSeWZ488"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. 학습 및 평가**"
      ],
      "metadata": {
        "id": "6Gsc0U58aIXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=N_EPOCHS,\n",
        "    gpus=1 if torch.cuda.is_available() else 0,\n",
        "    progress_bar_refresh_rate=30\n",
        ")\n",
        "\n",
        "trainer.fit(model, data_module)\n",
        "trainer.test(model, datamodule=data_module)"
      ],
      "metadata": {
        "id": "2SzP1YSpaIF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. 사용자 입력을 통한 요약 수행**"
      ],
      "metadata": {
        "id": "YE0enyEfaRbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 입력 받기\n",
        "user_input = input(\"요약할 텍스트를 입력하세요: \")\n",
        "\n",
        "# 요약 수행\n",
        "def summarize_text(text):\n",
        "    tokenized_text = tokenizer.encode(text, return_tensors='pt', max_length=MAX_TOKEN_COUNT, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        summary = model(tokenized_text, tokenized_text, tokenized_text)\n",
        "    return summary\n",
        "\n",
        "summary = summarize_text(user_input)\n",
        "print(\"요약 결과:\", summary)"
      ],
      "metadata": {
        "id": "2X_82P-faWZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}